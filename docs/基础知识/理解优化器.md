# 理解优化器(Optimizer)

![无标题-2022-09-06-1805.excalidraw](https://notebook-media.oss-cn-beijing.aliyuncs.com/img/无标题-2022-09-06-1805.excalidraw.png)

本文与其他介绍optimizer有所不同的是：

1. 本文完整的阐述了批量学习、在线学习两条路线的optimizer进化史
2. 本文只从感知上进行理解，不涉及数学推导，着重于记忆与理解，而非严肃的论文体
3. 本文为了从理解入手, 改变了原论文中的公式形式



## 文本结构

[TOC]

## 源头：SGD

$$
w _ { t + 1 } = w _ { t } - \eta\cdot g _ { t }
$$

$\eta$：学习率

每次使用一个样本计算损失函数, 从而得到梯度,  逐步的迭代参数



## 批量学习优化器

使用一个样本容易陷入局部最优点, 且迭代速度慢,容易产生震荡,批量学习正是解决这个问题的方法.

当然在线学习有另一条路子解决,且在线学习是为了参数稀疏化而生的,更适用于搜广推领域.



### momentum

$$
w_{t+1}=w_{t}- \eta \cdot(\beta _{1}\cdot m_{t-1}+(1- \beta _{1})\cdot g_{t})
$$

momentum在SGD的基础上加上了**惯性**,  即如与上一步方向一致,加快速度,与上一步方向不一致减慢.但当局部最优点很深时,动量也无法拯救.

### NAG 

$$
{ v _ { t } = \alpha v _ { t - 1 } + \eta\nabla J ( W _ { t } - \alpha v _ { t -1} ) }
$$

$$
{ W _ { t + 1 } = W _ { t } - v _ { t } }
$$

NAG在SGD的基础上加入了**看一步走一步**的机制,   即将提前计算一步梯度,  防止小球盲目跟随上一步动量,冲出最优点.

### AdaGrad

$$
w_{t+1}=w_{t}- \frac {\eta_0} {\sqrt{\sum _{i =1}^{t}g_i^{2}+\varepsilon}}\cdot g_{t}
$$

$\varepsilon$：极小的数,防止分母为 0

前面的优化器的学习率都是固定的, 而AdaGrad可随着模型的收敛**自适应学习率**. 即更新频率高的维度学习率逐渐减缓,  更新频率低的维度维持高学习率.

### RMSProp

$$
w_{t+1}=w_{t}- \frac {\eta_0} {E \left[ g^{2}\right] _{t}+\varepsilon}\cdot g_{t}
$$

AdaGrad 暴力的随着梯度累计越高, 学习率越低.  RMS对此做了改进,只取一个窗口下的梯度平均值.

### AdaDelta

$$
w_{t+1}=w_{t}- \Delta w_{t}
$$

$$
\Delta w_{t}=\frac {E[\Delta w]_{t-1}} {E \left[ g^{2}\right] _{t}+\varepsilon}\cdot g_{t}
$$

AdaGrad 和 RMSProp 都得初始化学习率,可不可以连初始化都不用呢?AdaDelta给了答案

### Adam

$$
w_{t+1}=w_{t}- \frac{\alpha} { \sqrt{\beta _{2}\cdot V_{t-1}+(1- \beta _{2})g_{t}^{2}}} \cdot  {(\beta _{1}\cdot m_{t-1}+(1- \beta _{1})\cdot g_{t})}
$$

$$
m_{t}= \beta _{1}\cdot m_{t-1}+(1- \beta _{1})\cdot g_{t}
$$

$$
V_{t}= \beta _{2}\cdot V_{t-1}+(1- \beta _{2})g_{t}^{2}
$$

可以理解为**动量+学习率**自适应





### 可视化对比

![](https://notebook-media.oss-cn-beijing.aliyuncs.com/img/20180425221525155.gif)

![](https://notebook-media.oss-cn-beijing.aliyuncs.com/img/20180426130002689.gif)



### 进展

1. [SWATS](https://arxiv.org/pdf/1712.07628.pdf)  2017
2. [AmsGrad](http://www.satyenkale.com/papers/amsgrad.pdf)  2018
3. [AdaBound](https://openreview.net/pdf?id=Bkg3g2R9FX)  2019
4. [RAdam](https://arxiv.org/pdf/1908.03265.pdf) 2019
5. [AdamW](https://arxiv.org/pdf/1711.05101.pdf) 2019
6. [RAdam](https://arxiv.org/pdf/1908.03265.pdf)  2020



## 在线学习优化器

在线学习的优化器用于流式数据的训练，对于数据量相当庞大，且数据相对稀疏的场景尤为合适。

### TG

#### 简单截断法

如何训练出稀疏性的参数呢？简单的想法就是设置一个阈值，小于这个阈值直接置零。
$$
W^{(t+1)}=T_{0}(W^{(t)}- \eta ^{(t)}G^{(t)}, \theta)
$$

$$
T_{0}(v_{i}\theta)= \left\{ \begin{matrix} 0 \quad if|v_{i}| \leq \theta \\ v_{i}\quad otherwise \\ \end{matrix} \right.
$$

如此暴力的做法会导致一些训练不足的参数置零，于是就有了优化版本：

#### 梯度截断法

$$
f(w_{i})=T_{1}(w_{i}- \eta \nabla _{1}L(w_{i},z_{i}), \eta g_{i}, \theta)
$$

$$
T_{1}(v_{j}, \alpha , \theta)=\left\{
								\begin{array}{ll}
{  { max } ( 0 , v _ { j } - \alpha ) } & {  v _ { j } \in [0,\theta]} \\ 
{ min ( 0 , v _ { j } + \alpha ) } & {  v _ { j } \in [-\theta,0] }  \\
v_{j}\quad otherwise
								\end{array}
    							\right.
$$

别被公式吓坏了，下图可更加直观的看出区别，梯度阶段法由两个参数控制稀疏度。直观的理解：参数接近0时，推波助澜至更接近0；更进一步接近0时，直接置零。这比简单截断法要顺滑一些。

![](https://notebook-media.oss-cn-beijing.aliyuncs.com/img/微信图片_20220909223706.jpg)



### FOBOS

$$
w_{t+ \frac{1}{2}}=w_{t}- \eta _{t}g_{t}^{f}
$$

$$
w _ { t + 1 } =  { argmin }_{w} \{ \frac { 1 } { 2 } | | w - w _ { t + \frac { 1 } { 2 } } | | ^ { 2 } + \eta _ { t + \frac { 1 } { 2 } } r ( w ) \}
$$

FOBOS将此问题拆解成了两部分，在梯度下降之后，加入了一个优化问题。即控制与梯度下降得到的结果不太远的第二范数，以及增加稀疏性的第一范数。

### RDA

$$
w_{t + 1} =  { argmin }_{w} \{ \frac { 1 } { t } \sum _ { r= 1 } ^ { t } \big \langle G ^ { r } , w \big \rangle + \lambda || w ||_1 + \frac { r } { 2 \sqrt{t} } ||w ||_2^2\}
$$

FOBOS只是拿了本步的梯度下降结果，在在线学习这种流式学习中，一次只学习一条数据，很容易造成参数的波动，参考的更远，可以减少这种异常点波动。

这不，RDA来了，RDA优化的第一项是梯度对参数的积分平均值，第二项是第一范数（增加稀疏性），第三项第二范数约束参数不离零太远.

### FTRL

$$
w _ { t + 1 } = {argmin}_{w}   \{ \frac { 1 } { t } \sum _ { r= 1 } ^ { t } \big \langle G ^ { r } , w \big \rangle+ \lambda || w ||_1 +\lambda || w ||_2^2 +\frac{1}{2}\sum _{s=1}^{t}\sigma ^{(s)} | | w - w _ { t+ \frac{1}{2}} | | ^ { 2 }  \}
$$

FTRL是RDA和FOBOS优点的合集,即又考虑与之前的参数距离,又考虑本步梯度下降的结果.

### 进展

1. [FTML](http://proceedings.mlr.press/v70/zheng17a.html)  2017
2. [SGDOL](https://arxiv.org/abs/1901.09068)  2019
3. [STORM](https://arxiv.org/abs/1905.10018)  2019
4. [EXP3](https://cseweb.ucsd.edu/~yfreund/papers/bandits.pdf)  2019
5. [UCB](ttps://parameterfree.com/2019/11/21/multi-armed-bandit-iv-ucb/)   2019

------

本文中的公式纯手敲，且为了理解的方便,没有写成原论文的公式形式,  如有错误之处可联系进行改正

<font size=24>关注本公众号，下期更精彩</font>

<img src="https://notebook-media.oss-cn-beijing.aliyuncs.com/img/image-20220930221129484.png" alt="image-20220930221129484" style="zoom: 80%;" />