# 理解范数惩罚正则化

> 本文着重于参数范数惩罚正则化方法的直观理解，而非严谨推导



## 罗列一下正则化方法

-  参数范数惩罚
  - L2 参数正则化 
  - L1 参数正则化 
- 作为约束的范数惩罚
- 正则化和欠约束问题
- 数据集增强
- 噪声鲁棒性
- 半监督学习
- 多任务学习
- 提前终止
- 参数绑定和参数共享
- 稀疏表
- Bagging 和其它集成方法
- Dropout 
- 对抗训练
- 切面距离、正切传播和流形正切分类器



## L1，L2正则的概率分布

频率派认为参数是一个常量，优化方法为最大似然MLE，优化目标为：  


$$
{ argmax }_ { \theta } P ( Y , X | \theta )
$$


如果误差服从高斯分布，则：

​    
$$
P(Y,X| \theta)= \prod _{i=1}^{n}\frac{1}{\sqrt{2 \pi}\sigma}exp \left\{ - \frac{1}{2 \sigma ^{2}}(y-f(x; \theta))^{2}\right\}
$$


取负对数之后，等价于一般的最小二乘法：   

$$
argmin_\theta \sum _{i=1}^{n}(y_{i}-f(x_{i}; \theta))^{2}
$$


贝叶斯派认为参数不是一个常量，而是一个分布：

​                           
$$
\theta \sim f ( \theta )
$$


此时进行最大似然：

$$
argmax_\theta P(Y,X| \theta)f(\theta)
$$


此时取负对数：

$$
 { argmin }[ - \log ( P ( Y , X | \theta )) - \log ( f ( \theta ) )]
$$


等价于：

$$
 { a r g m i n } \sum _ { i = 1 } ^ { n } ( y _ { i } - f ( x _ { i } ; \theta ) ) ^ { 2 } - \log ( f ( \theta ) )
$$


相比之前多了一项，而这一项就是正则项

1. 假设参数符合**拉普拉斯分布**，则对应了L1正则（Lasso回归）
2. 假设参数符合**高斯分布**，则对应了L2正则（Ridge回归）

限制了参数的分布空间，即控制了模型的空间，可有效控制模型的过拟合。



## 为什么L1更容易稀疏解



### 先验概率角度

<img src="https://notebook-media.oss-cn-beijing.aliyuncs.com/img/Snipaste_2022-09-19_16-07-44.png" alt="Snipaste_2022-09-19_16-07-44" style="zoom: 8%;" />

如图所示，拉普拉斯分布相比高斯分布更多的集中在0点附近，而高斯分布更加平滑。



### 图形解释角度

![](https://notebook-media.oss-cn-beijing.aliyuncs.com/img/v2-82ec609d0958df7ae138c1e08cbe05d6_r.png)

最小二乘法$$[y-x(w_1+w_2)]^2$$ 如果应用于二维，从图形上看是椭圆

（椭圆的一般方程$$ Ax^2 + By^2 + Cxy + Dx + Ey + F = 0$$）

如果参数是二维的$$[w_1,w_2]$$，那么它的$$L1$$范数是$$| w_1 | + | w_2 |$$。设$$| w_1 | + | w_2 |=c$$ 则可以画出菱形

同理$$L2$$范数为$$w_1^2 + w_2^2=c$$ ，即画出圆心为原点的圆

所以此优化问题可以可视化为上图，由图形可知$L1$更容易产生0解



### 导数角度

定义损失函数为$$L(w)$$，假设只有一个参数$$w$$，则加上$$L1$$正则的损失函数为：



$$
J _ { L 1 } ( w ) = L ( w ) + \lambda | w |
$$


加上$L2$正则的损失函数为：



$$
J_{L2}(w)=L(w)+ \lambda w^{2}
$$


此刻设$$L(w)$$在$$w=0$$时，导数为$$d$$，则加上$$J_{L1}(w)$$导数当$$w>0$$时等于： $$d+\lambda$$ ，当$$w>0$$ 时等于： $$d-\lambda$$ 

而加入$$J_{L2}(w)$$导数为$$d+2\lambda w$$ ，由于$$w=0$$，所以导数为$$d$$ 

所以$$J_{L2}(w)$$ 在经过点0前后，导数是不变的，而$$J_{L1}(w)$$是骤然变小的，更容易产生0值。




## 引用

1. 《Deep Learning》
1. 《PRML》



<font size=24>关注本公众号，下期更精彩</font>

<img src="https://notebook-media.oss-cn-beijing.aliyuncs.com/img/image-20220930221129484.png" alt="image-20220930221129484" style="zoom: 80%;" />

