{"./":{"url":"./","title":"Introduction","keywords":"","body":"介绍 本项目为公众号 无数据不智能 的文章集锦，公众号中有PDF可以下载 如果排版有问题请移步公众号 知识图谱技术前沿 从word2vec、主题模型、到bert，再到对比学习，一文搞懂来龙去脉 By 杜永安，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2022-11-19 11:48:58 "},"基础知识/理解优化器.html":{"url":"基础知识/理解优化器.html","title":"理解优化器","keywords":"","body":"理解优化器(Optimizer) 本文与其他介绍optimizer有所不同的是： 本文完整的阐述了批量学习、在线学习两条路线的optimizer进化史 本文只从感知上进行理解，不涉及数学推导，着重于记忆与理解，而非严肃的论文体 本文为了从理解入手, 改变了原论文中的公式形式 文本结构 [TOC] 源头：SGD wt+1=wt−η⋅gt\r w _ { t + 1 } = w _ { t } - \\eta\\cdot g _ { t }\r w​t+1​​=w​t​​−η⋅g​t​​ $\\eta$：学习率 每次使用一个样本计算损失函数, 从而得到梯度, 逐步的迭代参数 批量学习优化器 使用一个样本容易陷入局部最优点, 且迭代速度慢,容易产生震荡,批量学习正是解决这个问题的方法. 当然在线学习有另一条路子解决,且在线学习是为了参数稀疏化而生的,更适用于搜广推领域. momentum wt+1=wt−η⋅(β1⋅mt−1+(1−β1)⋅gt)\r w_{t+1}=w_{t}- \\eta \\cdot(\\beta _{1}\\cdot m_{t-1}+(1- \\beta _{1})\\cdot g_{t})\r w​t+1​​=w​t​​−η⋅(β​1​​⋅m​t−1​​+(1−β​1​​)⋅g​t​​) momentum在SGD的基础上加上了惯性, 即如与上一步方向一致,加快速度,与上一步方向不一致减慢.但当局部最优点很深时,动量也无法拯救. NAG vt=αvt−1+η∇J(Wt−αvt−1)\r { v _ { t } = \\alpha v _ { t - 1 } + \\eta\\nabla J ( W _ { t } - \\alpha v _ { t -1} ) }\r v​t​​=αv​t−1​​+η∇J(W​t​​−αv​t−1​​) Wt+1=Wt−vt\r { W _ { t + 1 } = W _ { t } - v _ { t } }\r W​t+1​​=W​t​​−v​t​​ NAG在SGD的基础上加入了看一步走一步的机制, 即将提前计算一步梯度, 防止小球盲目跟随上一步动量,冲出最优点. AdaGrad wt+1=wt−η0∑i=1tgi2+ε⋅gt\r w_{t+1}=w_{t}- \\frac {\\eta_0} {\\sqrt{\\sum _{i =1}^{t}g_i^{2}+\\varepsilon}}\\cdot g_{t}\r w​t+1​​=w​t​​−​√​∑​i=1​t​​g​i​2​​+ε​​​​​η​0​​​​⋅g​t​​ $\\varepsilon$：极小的数,防止分母为 0 前面的优化器的学习率都是固定的, 而AdaGrad可随着模型的收敛自适应学习率. 即更新频率高的维度学习率逐渐减缓, 更新频率低的维度维持高学习率. RMSProp wt+1=wt−η0E[g2]t+ε⋅gt\r w_{t+1}=w_{t}- \\frac {\\eta_0} {E \\left[ g^{2}\\right] _{t}+\\varepsilon}\\cdot g_{t}\r w​t+1​​=w​t​​−​E[g​2​​]​t​​+ε​​η​0​​​​⋅g​t​​ AdaGrad 暴力的随着梯度累计越高, 学习率越低. RMS对此做了改进,只取一个窗口下的梯度平均值. AdaDelta wt+1=wt−Δwt\r w_{t+1}=w_{t}- \\Delta w_{t}\r w​t+1​​=w​t​​−Δw​t​​ Δwt=E[Δw]t−1E[g2]t+ε⋅gt\r \\Delta w_{t}=\\frac {E[\\Delta w]_{t-1}} {E \\left[ g^{2}\\right] _{t}+\\varepsilon}\\cdot g_{t}\r Δw​t​​=​E[g​2​​]​t​​+ε​​E[Δw]​t−1​​​​⋅g​t​​ AdaGrad 和 RMSProp 都得初始化学习率,可不可以连初始化都不用呢?AdaDelta给了答案 Adam wt+1=wt−αβ2⋅Vt−1+(1−β2)gt2⋅(β1⋅mt−1+(1−β1)⋅gt)\r w_{t+1}=w_{t}- \\frac{\\alpha} { \\sqrt{\\beta _{2}\\cdot V_{t-1}+(1- \\beta _{2})g_{t}^{2}}} \\cdot {(\\beta _{1}\\cdot m_{t-1}+(1- \\beta _{1})\\cdot g_{t})}\r w​t+1​​=w​t​​−​√​β​2​​⋅V​t−1​​+(1−β​2​​)g​t​2​​​​​​​α​​⋅(β​1​​⋅m​t−1​​+(1−β​1​​)⋅g​t​​) mt=β1⋅mt−1+(1−β1)⋅gt\r m_{t}= \\beta _{1}\\cdot m_{t-1}+(1- \\beta _{1})\\cdot g_{t}\r m​t​​=β​1​​⋅m​t−1​​+(1−β​1​​)⋅g​t​​ Vt=β2⋅Vt−1+(1−β2)gt2\r V_{t}= \\beta _{2}\\cdot V_{t-1}+(1- \\beta _{2})g_{t}^{2}\r V​t​​=β​2​​⋅V​t−1​​+(1−β​2​​)g​t​2​​ 可以理解为动量+学习率自适应 可视化对比 进展 SWATS 2017 AmsGrad 2018 AdaBound 2019 RAdam 2019 AdamW 2019 RAdam 2020 在线学习优化器 在线学习的优化器用于流式数据的训练，对于数据量相当庞大，且数据相对稀疏的场景尤为合适。 TG 简单截断法 如何训练出稀疏性的参数呢？简单的想法就是设置一个阈值，小于这个阈值直接置零。 W(t+1)=T0(W(t)−η(t)G(t),θ)\r W^{(t+1)}=T_{0}(W^{(t)}- \\eta ^{(t)}G^{(t)}, \\theta)\r W​(t+1)​​=T​0​​(W​(t)​​−η​(t)​​G​(t)​​,θ) T0(viθ)={0if∣vi∣≤θviotherwise\r T_{0}(v_{i}\\theta)= \\left\\{ \\begin{matrix} 0 \\quad if|v_{i}| \\leq \\theta \\\\ v_{i}\\quad otherwise \\\\ \\end{matrix} \\right.\r T​0​​(v​i​​θ)=​⎩​⎨​⎧​​​0if∣v​i​​∣≤θ​v​i​​otherwise​​​ 如此暴力的做法会导致一些训练不足的参数置零，于是就有了优化版本： 梯度截断法 f(wi)=T1(wi−η∇1L(wi,zi),ηgi,θ)\r f(w_{i})=T_{1}(w_{i}- \\eta \\nabla _{1}L(w_{i},z_{i}), \\eta g_{i}, \\theta)\r f(w​i​​)=T​1​​(w​i​​−η∇​1​​L(w​i​​,z​i​​),ηg​i​​,θ) T1(vj,α,θ)={max(0,vj−α)vj∈[0,θ]min(0,vj+α)vj∈[−θ,0]vjotherwise\r T_{1}(v_{j}, \\alpha , \\theta)=\\left\\{\r \t\t\t\t\t\t\t\t\\begin{array}{ll}\r { { max } ( 0 , v _ { j } - \\alpha ) } & { v _ { j } \\in [0,\\theta]} \\\\ \r { min ( 0 , v _ { j } + \\alpha ) } & { v _ { j } \\in [-\\theta,0] } \\\\\r v_{j}\\quad otherwise\r \t\t\t\t\t\t\t\t\\end{array}\r \t\t\t\t\t\t\t\\right.\r T​1​​(v​j​​,α,θ)=​⎩​⎨​⎧​​​max(0,v​j​​−α)​min(0,v​j​​+α)​v​j​​otherwise​​​v​j​​∈[0,θ]​v​j​​∈[−θ,0]​​ 别被公式吓坏了，下图可更加直观的看出区别，梯度阶段法由两个参数控制稀疏度。直观的理解：参数接近0时，推波助澜至更接近0；更进一步接近0时，直接置零。这比简单截断法要顺滑一些。 FOBOS wt+12=wt−ηtgtf\r w_{t+ \\frac{1}{2}}=w_{t}- \\eta _{t}g_{t}^{f}\r w​t+​2​​1​​​​=w​t​​−η​t​​g​t​f​​ wt+1=argminw{12∣∣w−wt+12∣∣2+ηt+12r(w)}\r w _ { t + 1 } = { argmin }_{w} \\{ \\frac { 1 } { 2 } | | w - w _ { t + \\frac { 1 } { 2 } } | | ^ { 2 } + \\eta _ { t + \\frac { 1 } { 2 } } r ( w ) \\}\r w​t+1​​=argmin​w​​{​2​​1​​∣∣w−w​t+​2​​1​​​​∣∣​2​​+η​t+​2​​1​​​​r(w)} FOBOS将此问题拆解成了两部分，在梯度下降之后，加入了一个优化问题。即控制与梯度下降得到的结果不太远的第二范数，以及增加稀疏性的第一范数。 RDA wt+1=argminw{1t∑r=1t⟨Gr,w⟩+λ∣∣w∣∣1+r2t∣∣w∣∣22}\r w_{t + 1} = { argmin }_{w} \\{ \\frac { 1 } { t } \\sum _ { r= 1 } ^ { t } \\big \\langle G ^ { r } , w \\big \\rangle + \\lambda || w ||_1 + \\frac { r } { 2 \\sqrt{t} } ||w ||_2^2\\}\r w​t+1​​=argmin​w​​{​t​​1​​∑​r=1​t​​⟨G​r​​,w⟩+λ∣∣w∣∣​1​​+​2√​t​​​​​r​​∣∣w∣∣​2​2​​} FOBOS只是拿了本步的梯度下降结果，在在线学习这种流式学习中，一次只学习一条数据，很容易造成参数的波动，参考的更远，可以减少这种异常点波动。 这不，RDA来了，RDA优化的第一项是梯度对参数的积分平均值，第二项是第一范数（增加稀疏性），第三项第二范数约束参数不离零太远. FTRL wt+1=argminw{1t∑r=1t⟨Gr,w⟩+λ∣∣w∣∣1+λ∣∣w∣∣22+12∑s=1tσ(s)∣∣w−wt+12∣∣2}\r w _ { t + 1 } = {argmin}_{w} \\{ \\frac { 1 } { t } \\sum _ { r= 1 } ^ { t } \\big \\langle G ^ { r } , w \\big \\rangle+ \\lambda || w ||_1 +\\lambda || w ||_2^2 +\\frac{1}{2}\\sum _{s=1}^{t}\\sigma ^{(s)} | | w - w _ { t+ \\frac{1}{2}} | | ^ { 2 } \\}\r w​t+1​​=argmin​w​​{​t​​1​​∑​r=1​t​​⟨G​r​​,w⟩+λ∣∣w∣∣​1​​+λ∣∣w∣∣​2​2​​+​2​​1​​∑​s=1​t​​σ​(s)​​∣∣w−w​t+​2​​1​​​​∣∣​2​​} FTRL是RDA和FOBOS优点的合集,即又考虑与之前的参数距离,又考虑本步梯度下降的结果. 进展 FTML 2017 SGDOL 2019 STORM 2019 EXP3 2019 UCB 2019 本文中的公式纯手敲，且为了理解的方便,没有写成原论文的公式形式, 如有错误之处可联系进行改正 关注本公众号，下期更精彩 By 杜永安，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2022-11-19 11:13:51 "},"基础知识/理解范数惩罚正则化.html":{"url":"基础知识/理解范数惩罚正则化.html","title":"理解范数惩罚正则化","keywords":"","body":"理解范数惩罚正则化 本文着重于参数范数惩罚正则化方法的直观理解，而非严谨推导 罗列一下正则化方法 参数范数惩罚 L2 参数正则化 L1 参数正则化 作为约束的范数惩罚 正则化和欠约束问题 数据集增强 噪声鲁棒性 半监督学习 多任务学习 提前终止 参数绑定和参数共享 稀疏表 Bagging 和其它集成方法 Dropout 对抗训练 切面距离、正切传播和流形正切分类器 L1，L2正则的概率分布 频率派认为参数是一个常量，优化方法为最大似然MLE，优化目标为： argmaxθP(Y,X∣θ)\r { argmax }_ { \\theta } P ( Y , X | \\theta )\r argmax​θ​​P(Y,X∣θ) 如果误差服从高斯分布，则： ​P(Y,X∣θ)=∏i=1n12πσexp{−12σ2(y−f(x;θ))2}\r P(Y,X| \\theta)= \\prod _{i=1}^{n}\\frac{1}{\\sqrt{2 \\pi}\\sigma}exp \\left\\{ - \\frac{1}{2 \\sigma ^{2}}(y-f(x; \\theta))^{2}\\right\\}\r P(Y,X∣θ)=∏​i=1​n​​​√​2π​​​σ​​1​​exp{−​2σ​2​​​​1​​(y−f(x;θ))​2​​} 取负对数之后，等价于一般的最小二乘法： argminθ∑i=1n(yi−f(xi;θ))2\r argmin_\\theta \\sum _{i=1}^{n}(y_{i}-f(x_{i}; \\theta))^{2}\r argmin​θ​​∑​i=1​n​​(y​i​​−f(x​i​​;θ))​2​​ 贝叶斯派认为参数不是一个常量，而是一个分布： ​θ∼f(θ)\r \\theta \\sim f ( \\theta )\r θ∼f(θ) 此时进行最大似然： argmaxθP(Y,X∣θ)f(θ)\r argmax_\\theta P(Y,X| \\theta)f(\\theta)\r argmax​θ​​P(Y,X∣θ)f(θ) 此时取负对数： argmin[−log(P(Y,X∣θ))−log(f(θ))]\r { argmin }[ - \\log ( P ( Y , X | \\theta )) - \\log ( f ( \\theta ) )]\r argmin[−log(P(Y,X∣θ))−log(f(θ))] 等价于： argmin∑i=1n(yi−f(xi;θ))2−log(f(θ))\r { a r g m i n } \\sum _ { i = 1 } ^ { n } ( y _ { i } - f ( x _ { i } ; \\theta ) ) ^ { 2 } - \\log ( f ( \\theta ) )\r argmin∑​i=1​n​​(y​i​​−f(x​i​​;θ))​2​​−log(f(θ)) 相比之前多了一项，而这一项就是正则项 假设参数符合拉普拉斯分布，则对应了L1正则（Lasso回归） 假设参数符合高斯分布，则对应了L2正则（Ridge回归） 限制了参数的分布空间，即控制了模型的空间，可有效控制模型的过拟合。 为什么L1更容易稀疏解 先验概率角度 如图所示，拉普拉斯分布相比高斯分布更多的集中在0点附近，而高斯分布更加平滑。 图形解释角度 最小二乘法[y−x(w1+w2)]2[y-x(w_1+w_2)]^2[y−x(w​1​​+w​2​​)]​2​​ 如果应用于二维，从图形上看是椭圆 （椭圆的一般方程Ax2+By2+Cxy+Dx+Ey+F=0 Ax^2 + By^2 + Cxy + Dx + Ey + F = 0Ax​2​​+By​2​​+Cxy+Dx+Ey+F=0） 如果参数是二维的[w1,w2][w_1,w_2][w​1​​,w​2​​]，那么它的L1L1L1范数是∣w1∣+∣w2∣| w_1 | + | w_2 |∣w​1​​∣+∣w​2​​∣。设∣w1∣+∣w2∣=c| w_1 | + | w_2 |=c∣w​1​​∣+∣w​2​​∣=c 则可以画出菱形 同理L2L2L2范数为w12+w22=cw_1^2 + w_2^2=cw​1​2​​+w​2​2​​=c ，即画出圆心为原点的圆 所以此优化问题可以可视化为上图，由图形可知$L1$更容易产生0解 导数角度 定义损失函数为L(w)L(w)L(w)，假设只有一个参数www，则加上L1L1L1正则的损失函数为： JL1(w)=L(w)+λ∣w∣\r J _ { L 1 } ( w ) = L ( w ) + \\lambda | w |\r J​L1​​(w)=L(w)+λ∣w∣ 加上$L2$正则的损失函数为： JL2(w)=L(w)+λw2\r J_{L2}(w)=L(w)+ \\lambda w^{2}\r J​L2​​(w)=L(w)+λw​2​​ 此刻设L(w)L(w)L(w)在w=0w=0w=0时，导数为ddd，则加上JL1(w)J_{L1}(w)J​L1​​(w)导数当w>0w>0w>0时等于： d+λd+\\lambdad+λ ，当w>0w>0w>0 时等于： d−λd-\\lambdad−λ 而加入JL2(w)J_{L2}(w)J​L2​​(w)导数为d+2λwd+2\\lambda wd+2λw ，由于w=0w=0w=0，所以导数为ddd 所以JL2(w)J_{L2}(w)J​L2​​(w) 在经过点0前后，导数是不变的，而JL1(w)J_{L1}(w)J​L1​​(w)是骤然变小的，更容易产生0值。 引用 《Deep Learning》 《PRML》 关注本公众号，下期更精彩 By 杜永安，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2022-11-19 11:18:51 "},"少样本学习/少样本多标签分类.html":{"url":"少样本学习/少样本多标签分类.html","title":"少样本多标签分类","keywords":"","body":"利用元信息和对比学习解决零样本多标签分类问题 本篇为少样本多标签文本分类系列文章的第一篇 [TOC] 有监督多标签分类局限性 人工标注价格昂贵且耗时长，特别是当标签空间很大的时候。 只能处理训练集中出现的标签，一旦出现新的标签就得重新训练。 倾向于训练集中频繁出现的标签，对于尾部标签效果不好。 推理阶段 利用BM25进行标签召回，注意这里标签加入了标签描述。 利用bert进行相似度重排序，计算文章和标签及描述之间的相关性。 训练阶段 元信息：作者、地域、引用等，不同的场景可以有不同的元信息，本篇当中的数据集为论文。 利用元信息可以天然建立哪些文档更相近。 构建异构图 异构图：包含不同类型节点或边的图 PAP：paper–author–paper 同一个作者写的 PP：论文之间的引用 …… 训练的架构 分别在交互式和特征式两种进行实验，结果显示大部分情况下交互式效果更好 利用对比学习，拉大正负样本的距离。例如下图中两篇文档有共同的两个作者比随机的更加相似。 损失函数 −logexp(cos(ed,ed+)/τ)exp(cos(ed,ed+)/τ)+∑i=1Nexp(cos(ed,edi−)/τ\r - \\log \\frac { e x p ( \\cos ( e _d , e _ { d^+ } ) / \\tau ) } { e x p ( \\cos ( e_d , e _ { d ^+} ) / \\tau ) + \\sum _ { i = 1 } ^ { N }e x p ( \\cos ( e_d , e _ { d_i ^-} ) / \\tau }\r −log​exp(cos(e​d​​,e​d​+​​​​)/τ)+∑​i=1​N​​exp(cos(e​d​​,e​d​i​−​​​​)/τ​​exp(cos(e​d​​,e​d​+​​​​)/τ)​​ 实验 一点值得注意的地方 作者对元信息构建的不同路径进行了实验，发现叠加几条路径效果并没有得到显著提高，有些还是下降的。这也许是因为不同路径得出的结论是矛盾的。 引用 （2022.3）Metadata-Induced Contrastive Learning for Zero-Shot Multi-Label Text Classification ​ github:https://github.com/yuzhimanhua/MICoL 关注本公众号，下期更精彩 By 杜永安，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2022-11-19 11:14:43 "},"搜索和问答/搜索与问答算法架构的异同.html":{"url":"搜索和问答/搜索与问答算法架构的异同.html","title":"搜索与问答算法架构的异同","keywords":"","body":"BM25是否该淘汰了？搜索和问答傻傻分不清楚？本篇带你刨析文本匹配技术，解开这些谜底——基于检索的智能问答（IRQA）和搜索引擎背后的故事 [TOC] 搜索和问答的异同 基于检索的问答系统（IRQA）是指提前预设问答对，通过匹配问题返回预设答案的技术。 智能问答技术大致分为IRQA、KBQA、MRC，在之后的文章中会逐步介绍。 搜索引擎与问答系统中的语义匹配大同小异。 【问答大多长文本，搜索大多短文本？】——其实并非都是如此，实际场景是用户短文本长文本都有，无论是问答还是搜索两种都得做支撑，具体还是看场景。 【问答只是语义匹配，搜索有些会有个性化？】——其实也并非都是如此，有些场景的问答需要根据用户历史的喜好返回不同的内容，如“XX，放首歌给我”，随机放可不会满足用户需求。当然搜索的排序建模是从根上个性化的，问答这种场景的个性化往往是命中这个意图之后，调用另一套推荐系统。 要非得找个区别，那就问答只返回一个，搜索返回多个把！（笑），但实际上，当不能很好的匹配问题库时，会返回一些相关的推荐问题，这也不是返回一个了吧。 所以从表象上，搜索和问答三方面的特征各自都是具有的；但从内核上搜索的目的更多偏向点击率提高而非真的从语义上最相似。 接下来将延续以往的风格【能用俩字就不用仨字】阐述文本匹配技术 关键词召回 为什么都到了预训练大模型时代，还需要关键词召回？David Rau and Jaap Kamps做了相关实验，实验表明： 对于高度相关的文档有17%（CE@100: 12% + CE@500: 5%）的文档，被排在后面，而BM25排在top10，相关的也有类似的结论。 有一些高度相关的文档两种方法都排在很后面 神经网络软匹配能力虽强，但query term这种精确匹配能力却远远比不上BM25。David Rau and Jaap Kamps通过把文档中的非query term用[mask]掩码进行实验得到此结论。 ​ TREC 2020 Deep Learning Track数据集实验 ​ 保持原样和非query term用[mask]掩码对比 BM25 关键词目前最常用的是BM25，BM25由三部分组成，【词权重】、【词与文档相关性】、【词与query相关性】 score(Q,D)=∑qi∈QWqi⋅score(qi,D)⋅score(qi,Q)\r score(Q,D)=\\sum_{q_i \\in Q}W_{q_i} \\cdot score(q_i,D) \\cdot score(q_i,Q)\r score(Q,D)=∑​q​i​​∈Q​​W​q​i​​​​⋅score(q​i​​,D)⋅score(q​i​​,Q) 词权重取$IDF(q_i)$ ，$N$为文档总数，$n(q_i)$ 为出现$q_i$的文档数，$|D|$为文档长度，$avgdl$为文档平均长度，$f(q_i,d)$为在文档中的词频，$f(q_i,Q)$为在文档中的词频，$k_1,k_3,b$为调节参数。 score(Q,D)=∑qi∈Qln(N−n(qi)+0.5n(qi)+0.5+1)⋅(k1+1)f(qi,D)k1(1−b)+b⋅∣D∣avgdl+f(qi,D)⋅(k3+1)f(qi,Q)k3+f(qi,Q)\r score(Q,D)= \\sum _{q_i \\in Q} \\ln (\\frac{N-n(q_{i})+0.5}{n(q_{i})+0.5}+1) \\cdot \\frac{(k_{1}+1)f(q_i,D)}{k_{1}(1-b)+b \\cdot \\frac{|D|}{avgdl}+f(q_i,D)}\\cdot \\frac{(k_{3}+1)f(q_i,Q)}{k_{3}+f(q_i,Q)}\r score(Q,D)=∑​q​i​​∈Q​​ln(​n(q​i​​)+0.5​​N−n(q​i​​)+0.5​​+1)⋅​k​1​​(1−b)+b⋅​avgdl​​∣D∣​​+f(q​i​​,D)​​(k​1​​+1)f(q​i​​,D)​​⋅​k​3​​+f(q​i​​,Q)​​(k​3​​+1)f(q​i​​,Q)​​ 由公式，当b等于0时，词与文档匹配分数与文档长度无关；当$k_1$等于0时，词与文档相关性与文档中词频无关；当$k_3$等于0时，词与query相关性与query中词频无关无关。其中词与query相关性$k_3$等于0（蓝色），$k_3$等于2（红色）如下图所示： 根据业务的不同，调节相应参数，同时利用$bool$查询综合多种查询策略，例如：match、match_phrase、match_phrase_prefix、term等以适应业务 ES拓展近义词 config目录下创建analysis文件夹创建自己的同义词文件synonym.txt A,B=>C D,E D,E出现D和E都会同时检索D、E；出现A，B都会替换为C。 重启ES 创建索引库 { \"settings\": { \"index\": { \"analysis\": { \"analyzer\": { \"my_analyzer\": { \"tokenizer\": \"smartcn\", \"filter\": [\"my_synonym\"] } }, \"filter\": { \"my_synonym\": { \"type\": \"synonym\", \"synonyms_path\": \"analysis/synonym.txt\" } } } } }, \"mappings\": { \"properties\": { \"title\": { \"type\": \"text\", \"analyzer\": \"my_analyzer\" } } } } 语义召回 关于如何得到句子向量，笔者曾经用很长的篇幅阐述了句子表征的来龙去脉【？？？？？？？】，这里就不再赘述了。语义召回可以有多路，例如一路simcse，一路主题模型 特征式 由于召回是面向所有数据集的，交互式计算相似度复杂度高，合适的方案是用特征式之后使用faiss/annoy/milvus进行向量检索。 所以语义召回层更合适的模型如bert-whitening等由原始bert衍生的以及simcse等对比学习系列模型。具体的还是看这篇文章【？？？？？？？】。 交互式 如果想使用交互式，也是有方案可寻的。使用矩阵分解的方式，分解交互式打分函数s(q,k)≈∑u∈U,v∈Vf(q,u)g(u,v)h(v,k)s ( q , k ) \\approx \\sum _ { u \\in \\mathcal U , v \\in \\mathcal V } f ( q , u ) g ( u , v ) h ( v , k )s(q,k)≈∑​u∈U,v∈V​​f(q,u)g(u,v)h(v,k) （q:query k:document $q \\in \\mathcal {Q} \\quad k\\in \\mathcal {K}$） 即S≈FGHS \\approx FGHS≈FGH ($F\\in \\mathbb{R} ^{|Q|×|U|}、G\\in \\mathbb{R} ^{|U|×|V|}、H \\in \\mathbb{R} ^{|V|×|K|}$) 将$u$视为$\\mathcal K$的一部分(即某些列，选取一些有代表性的文档)；将$v$视为$\\mathcal Q$的一部分（即某些行，选取一些有代表性的query），于是： S≈F(F∩H)†HS \\approx F ( F\\cap H ) ^ { \\dagger } HS≈F(F∩H)​†​​H $F∩H$是指$F$某些行$H$某些列交集的元素拼成的矩阵，即$G=( F\\cap H ) ^ { \\dagger }$ 所以可以提前计算G和H，查询时，只需要计算q与u的相似度（$|U|$次计算），再与GH相乘即可。选取有代表性的文档和query可使用聚类的方式。 排序 由于搜索和问答的数据来源以及对个性化的要求不同，排序的建模有所不同。 问答 合并之前的关键词和语义召回数，问答利用用户的配置的问答对语料进行有监督训练。一个更精确的交互式的模型更符合这一层的设置，例如可以选直接用bert [CLS]位置作为输出，输入使用【句子1+\\+句子2】进行fine-turn。 class SimModel(torch.nn.Module): def __init__(self,model_name='bert-base-chinese',freeze_bert=False): super(SimModel,self).__init__() self.bert = BertModel.from_pretrained(model_name) if freeze_bert: for p in self.bert.parameters(): p.requires_grad=False self.linear = torch.nn.Linear(768,1) def forward(self,input_ids,attention_mask): outputs = self.bert(input_ids, attention_mask=attention_mask) sentences_embeddings=outputs.pooler_output.squeeze(1) out=self.linear(sentences_embeddings).squeeze(1) return out 搜索 搜索场景非常宽泛，不同场景有不同的策略，但更多的场景是为了提升点击率，更多的倾向于推荐系统，而非纯NLP。 搜索的训练数据主要来源于埋点，天然具有个性化。数据量超大时，需要分成两层粗排、精排。 搜索的召回中一般也会加入一些个性化召回如FM万能油。 粗排可以是精排的蒸馏，也可以直接训练一个轻量级的排序模型如LR省心小宝贝。 粗排的作用此时相当于对数据进行了分层，之后对每一层进行精排。 引用 信息检索导论 How Different are Pre-trained Transformers for Text Ranking? https://en.wikipedia.org/wiki/Okapi_BM25 句子表征 前世今生 https://spaces.ac.cn/archives/9336 关注本公众号，下期更精彩 By 杜永安，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2022-11-19 11:14:37 "},"相似度计算/主题模型大全LSA PLSA LDA HDP lda2vec.html":{"url":"相似度计算/主题模型大全LSA PLSA LDA HDP lda2vec.html","title":"主题模型大全LSA PLSA LDA HDP Lda2vec","keywords":"","body":"主题模型大全LSA PLSA LDA HDP lda2vec [TOC] 主题模型 所有主题模型都基于相同的假设： 每个文档包含多个主题 每个主题包含多个单词 LSA 将文章X单词矩阵进行SVD分解，分解为文章（句子）X主题、主题X主题、主题X单词单个矩阵，其中文章（句子）X主题作为文章（句子）向量。 PLSA d和 w 是已经观测到的变量，而 z 是未知的变量（主题），和LSA的矩阵分解是对应的。 最大的矩形里装有N篇文档，每一篇文档 d 自身有个概率$P(D)$，从文档 d 到主题 z 概率分布$P(Z|D)$，随后从主题到词概率分布$P(W|Z)$，由此构成了 w 和 z 的联合概率分布$P(D,W)$，由此PLSA的参数量为 $zd+wz$ 。 用EM算法求解模型 LDA PLSA假设分布为固定参数，容易产生过拟合，LDA在起基础上，加入狄利克雷分布，相当于加入先验知识。训练样本量足够大，pLSA的效果可以等同LDA。 狄利克雷分布：分布的分布，Beta分布的多元推广为狄利克雷分布，Beta分布是伯努利分布，二项分布的共轭先验（先验分布是beta分布，后验分也是beta分布）。 Beta分布常用于AB测试当中，例如已知某页面点击率正常范围在0.2~0.35，点击率符合伯努利分布，点击率分布的分布符合Beta分布 α和β是dirichlet的参数，对于M篇文档都一样，用来控制每篇文档的两个概率分布 θ对于每一篇文档都一样，用来表征文档的主题分布 z是每个词的隐含主题，w是文档里的词 α和β是文集（corpus）级别的参数，所以在M篇文档外边 θ是文档级别的参数，在N个词盘子的外边 z和w是一一对应的，每个词对应一个隐含主题vec P(θ,z⃗,w⃗∣a,β)=P(θ∣α)∏n−1NP(zn∣θ)P(wn∣zm,β)\r P(\\theta , \\vec{z}, \\vec{w}|a, \\beta)=P(\\theta | \\alpha)\\prod _{n-1}^{N}P(z_{n}| \\theta)P(w_{n}|z_{m}, \\beta)\r P(θ,​z​⃗​​,​w​⃗​​∣a,β)=P(θ∣α)∏​n−1​N​​P(z​n​​∣θ)P(w​n​​∣z​m​​,β) 下图大等边三角形内所有点到顶点对边距离和为1，代表生成三个单词的概率 小等边三角形内所有点到顶点对边距离代表生成三个主题的概率 小三角形中每个带叉的点都是一个PLSA模型，生成文档时，先根据叉点选择一个主题，再根据主题选择词 而LDA不再像PLSA是随机的点，而是小三角形中的曲线（一系列的点），即服从一定的分布，分布曲线由$\\alpha \\space \\beta$ 决定 HDP-LDA 在LDA中，主题的个数是一个预先指定的超参数。通常可通过验证集和训练集得到最佳超参数。即当训练集评价指标下降，但验证集开始上升时，到达最优点。但有时最优个数会超大，此时可选择评价指标下降速度变慢的点。具体的评价指标（困惑度）如下： perplexity(D)=exp[−∑d=1Mlogp(wi)∑d=1NNd}\r perplexity(D)=exp \\left[ -\\frac{\\sum _{d=1}^{M}\\log {p}(w_{i})}{\\sum _{d=1}^{N}N_{d}}\\right\\}\r perplexity(D)=exp[−​∑​d=1​N​​N​d​​​​∑​d=1​M​​logp(w​i​​)​​} 文档集合D，其中M为文档的总数， $w_d$ 为文档d中单词所组成的词袋向量，$ p(w_d)$ 为模型所预测的文档d的生成概率， $N_d$ 为文档d中单词的总数。 另外一种方法是在LDA基础之上融入分层狄利克雷过程（Hierarchical Dirichlet Process，HDP），构成一种非参数主题模型HDP-LDA。不需要提前预制主题个数，但模型参数求解更复杂。 HDP构造过程 类似于一个中国餐馆（一个文档），每个餐桌代表一个类别，从第一个顾客（一个单词）进来选桌子开始到最后一个结束，即可得出类别数。 n为已经选择好类别的单词的个数，$nj$为第 j 个类别的单词数，下一个单词被分到已存在类别的概率： $\\frac { n { j } } { n + \\alpha }$，新类别的概率： $\\frac{\\alpha}{n+ \\alpha}$。可知 $\\alpha$ 越大，选择已存在类别概率越小，最终类别数越多；第 j 个类别的单词数越多，下个单词分到该类别可能性越大。 lda2vec 其实就是：word2vec + lda，lda的训练也变成了深度学习的方式，初始文档主题分布矩阵，在输出层文档主题的输出加上word2vec输出。 引用 https://multithreaded.stitchfix.com/blog/2016/05/27/lda2vec/#topic=38&lambda=1&term= https://jmlr.org/papers/volume3/blei03a/blei03a.pdf 统计自然语言处理 关注本公众号，下期更精彩 By 杜永安，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2022-11-19 11:13:15 "},"相似度计算/原始bert参数如何有效生成句向量.html":{"url":"相似度计算/原始bert参数如何有效生成句向量.html","title":"原始Bert参数如何有效生成句向量","keywords":"","body":"原始bert参数如何有效生成句向量 目录 [TOC] BERT CLS 直接使用[CLS]位置向量作为句向量 last average 最后一层词向量（即除[CLS]之外）求平均 first last average 第一层和最后一层词向量平均 SBERT sbert采用经典的双塔方式，提出了三种目标函数： 拼接两个句子向量，直接二分类（Classification Objective Function） softmax(Wt(u,v,∣u−v∣))\r softmax(W_{t}(u,v,|u-v|))\r softmax(W​t​​(u,v,∣u−v∣)) 选择$(u,v,|u-v|)$拼接，mean pooling方式效果最好 利用两个句子的余弦距离的mean-squared-error作为损失函数（Regression Objective Function） 以下为论文中的实现 class CosineSimilarityLoss(nn.Module): def __init__(self, model: SentenceTransformer, loss_fct = nn.MSELoss(), cos_score_transformation=nn.Identity()): super(CosineSimilarityLoss, self).__init__() self.model = model self.loss_fct = loss_fct self.cos_score_transformation = cos_score_transformation def forward(self, sentence_features: Iterable[Dict[str, Tensor]], labels: Tensor): embeddings = [self.model(sentence_feature)['sentence_embedding'] for sentence_feature in sentence_features] output = self.cos_score_transformation(torch.cosine_similarity(embeddings[0], embeddings[1])) return self.loss_fct(output, labels.view(-1)) 每次给一个正例，一个负例，增大负例距离，减小正例距离（Triplet Objective Function）。$s_x$为句子向量$||\\cdot||$是一种距离衡量，间隔 $\\epsilon$ 保证正例$s_p$(positive) 比负例 $s_n$(negative) 至少要近$\\epsilon$。 max(∣∣sa−sp∣∣−∣∣sa−sn∣∣+ϵ,0)\r \\max(||s_{a}-s_{p}||-||s_{a}-s_{n}||+ \\epsilon ,0)\r max(∣∣s​a​​−s​p​​∣∣−∣∣s​a​​−s​n​​∣∣+ϵ,0) 在STS数据的测评 无监督 在无监督领域实验表明无论是bert-cls还是bert-avg效果都还不如直接Glove GloVe与word2vec的区别 word2vec：根据上下文呢预测中间的词汇，或者根据中间的词汇预测上下文。 GloVe：构建词汇的共现矩阵，再进行类似PCA的操作 有监督 作者使用Regression Objective Function损失函数的方式进行训练得出以下测评： Bert-flow 从上文中我们知道，bert在没有fine-turn的情况下，无论使用CLS还是avg效果都很差。为啥呢？ CLS预训练的目标是判断两个句子是否存在延续性，【连续性和相似大相径庭】。 各向异性：各个维度衡量标准不一样(左图为各向异性，右图为各向同性) ​ 由下图可知word2vec各向分布较为一致，而transformer差异性很大。 ​ 各向异性会导致向量堆积在一起，余弦相似度都很高 bert 空间向量所属的坐标系并非标准正交基。 cos(x,y)=∑i=1dxiyi∑i=1dxi∑i=1dyi\r \\cos(x,y)= \\frac{\\sum _{i=1}^{d}x_{i}y_{i}}{\\sqrt{\\sum _{i=1}^{d}x_{i}}\\sqrt{\\sum _{i=1}^{d}y_{i}}}\r cos(x,y)=​√​∑​i=1​d​​x​i​​​​​√​∑​i=1​d​​y​i​​​​​​​∑​i=1​d​​x​i​​y​i​​​​ 余弦相似度计算方式是在标准正交基下成立的，如果如果符合标准正交基，语义学习的好，效果应该是好的。 标准正交基：一个内积空间的正交基（orthogonal basis）是元素两两正交的基。称基中的元素为基向量。假若，一个正交基的基向量的模长都是单位长度1，则称这正交基为标准正交基 如何解决呢？ bert-flow提出的解决方案是从各向异性入手的：将原始的bert句子向量空间转换到标准高斯分布空间。 $u$是bert空间向量（observe space），$z$是高斯空间向量（latent space），服从$p_z(z)$的先验分布。$f$是通过【训练】（固定bert参数，只迭代预训练转化函数参数）得到的从高斯空间转换到bert空间的可逆函数。 z∼pz(z),u=fϕ(z)\r z \\sim p_z(z),u=f_{\\phi}(z)\r z∼p​z​​(z),u=f​ϕ​​(z) 根据流模型（flow model）原理，bert空间转换为高斯空间如下（$det$：行列式；$f^{-1}$：$f$ 的逆函数）： pu(u)=pz(fϕ−1(u))∣det∂fϕ−1(u)∂u∣\r p_u(u)=p_z(f_{\\phi}^{-1}(u))|det \\frac{\\partial f_{\\phi}^{-1}(u)}{\\partial u}|\r p​u​​(u)=p​z​​(f​ϕ​−1​​(u))∣det​∂u​​∂f​ϕ​−1​​(u)​​∣ 最大化对数似然函数（log likelihood） maxϕEu=BERT(sentence),sentence∼D\r \\max_{\\phi} \\mathbb E_u=BERT(sentence),sentence \\sim \\mathcal D\r max​ϕ​​E​u​​=BERT(sentence),sentence∼D logpz(fϕ−1(u))+log∣det∂fϕ−1(u)∂u∣\r \\log p_z(f_{\\phi}^{-1}(u))+ \\log |det \\frac{\\partial f_{\\phi}^{-1}(u)}{\\partial u}|\r logp​z​​(f​ϕ​−1​​(u))+log∣det​∂u​​∂f​ϕ​−1​​(u)​​∣ 流模型（flow model） 定义原始数据分布为简单的高斯分布$\\pi(z)$，且$𝑥=G(z)$ 。转换之后的x服从$p_G(x)$分布。此时的目标函数最大化对数似然等于最小化KL散度。 一些基础知识 雅可比矩阵（Jacobian） 定义$𝑥=f(z)$ ,$f$为可逆函数即$z=f^{-1}(x)$。如果设x和z都是二维的，那么此时Jacobian matrix（以下简称$J_f$）如下图所示。 所以如果$z=[z1,z_2],x=[z_1+z_2,2z_2]$，根据$J{f}$的求偏导的规则可求得$J{f}$ ，同理可得$J{f^{-1}}$，同时可知$J{f}J{f^{-1}}=I$。 ​ 行列式（Determinant） 计算方式：D=∑(−1)ka1k1a2k2⋯ankD= \\sum(-1)^{k}a_{1k_{1}}a_{2k_{2}}\\cdots a_{nk}D=∑(−1)​k​​a​1k​1​​​​a​2k​2​​​​⋯a​nk​​ ​ ​ 行列式的几何意义其实是高维空间的“体积”。 ​ ​ ​ 随机变量的变量替换定理(Change of Variable Theorem) 定义z服从$\\pi(z)$，x服从$p(x)$分布，$𝑥=f(z)$ 。x可以z通过f得到，那么$\\pi(z)$和$p(x)$有什么关系呢？ ​ 假设$\\pi(z)$和$p(x)$都服从最简单的均匀分布（Uniform Distribution），$\\pi(z)\\sim U(0,1) \\ ,\\ p(x)\\sim U(1,3)$。为了保证积分面积都为1，则$\\pi(z)=1\\ ,\\ p(x)=0.5$，即$p(x^{\\prime})= \\frac{1}{2}\\pi(z^{\\prime})$。 ​ ​ 更一般的情况，复杂的分布可以取其中一小段视为均匀分布，由此可以得到$p(x^{\\prime})= \\pi(z^{\\prime})| \\frac{dz}{dx}|$。（有可能反向对应，要加绝对值） ​ ​ ​ 推广到二维的情况（别忘了，det的含义是面积）： ​ 经过以下的推到可以得到两个分布之间的推导公式： ​ ​ 所以将似然函数中$P_G$替换成分布之间的推导公式，将$z^i$替换为向量之间的推导公式，可得到最终的目标函数。 ​ Bert-whitening bert-flow为了能够转化分布，仍然需要训练一个转换参数。Bert-whitening采用了更加直接的办法，不需要再去迭代预训练，从标准正交基入手，利用白化的手段将bert向量转换到以标准正交基为基底的向量。 经过白化操作的数据具有以下特征： 消除了特征之间的相关性（协方差矩阵除对角线都为0） 所有特征的方差都为 1（协方差矩阵对角线都为1） PCA白化：$W= \\Lambda ^{-1/2}U^{T}$ ZCA白化：$W=U \\Lambda ^{-1/2}U^{T}$ 同：都消除了特征之间相关性，同时使得所有特征的方差都为 1。 异：相比于PCA白化，ZCA白化把数据旋转回了原来的特征空间，处理后的数据更接近原始数据。 设向量集合为$\\left{x{i}\\right} {i=1}^{N}$，标准的高斯分布均值为0，协方差为单位阵这与白化操作不谋而合。想要均值为0直接$xi-\\mu $，其中$\\mu= \\frac{1}{N}\\sum {i=1}^{N}x{i}$ 。接下来将$x_i-\\mu $ 执行线性变化$(x{i}- \\mu)W$，使得转换后的数据协方差矩阵为单位阵。 x~i=(xi−μ)W\r \\tilde{x}_{i}=(x_{i}- \\mu)W\r ​x​~​​​i​​=(x​i​​−μ)W 在白化操作中，$W$给出了解法： 矩阵$X$白化至矩阵$Y$，根据协方差求解公式，$X$协方差$D_X=\\frac{1}{m}XX^{T}$，所以： DY=1mYYT=1m(WX)(WX)T\r D_Y= \\frac{1}{m}YY^{T}= \\frac{1}{m}(WX)(WX)^{T}\r D​Y​​=​m​​1​​YY​T​​=​m​​1​​(WX)(WX)​T​​ =1mWXXTWT=W(1mXXT)WT=WDXWT\r = \\frac{1}{m}WXX^{T}W^{T}=W(\\frac{1}{m}XX^{T})W^{T}=WD_XW^{T}\r =​m​​1​​WXX​T​​W​T​​=W(​m​​1​​XX​T​​)W​T​​=WD​X​​W​T​​ 白化的目标是使得$\\tilde{\\Sigma}=W^{T}\\Sigma W=I$，所以可以推导出： ⇒Σ=(WT)−1W−1=(W−1)TW−1\r \\Rightarrow \\Sigma =(W^{T})^{-1}W^{-1}=(W^{-1})^{T}W^{-1}\r ⇒Σ=(W​T​​)​−1​​W​−1​​=(W​−1​​)​T​​W​−1​​ $\\Sigma$是一个半正定对称矩阵，数据够多时通常是正定的，从而$\\Sigma=U \\Lambda U^{T}$，$U$是正交矩阵，而$\\Lambda$是对角阵，并且对角线元素都是正的，那么可以令$W^{-1}= \\sqrt{\\Lambda}U^{T}$,即$W=U \\sqrt{\\Lambda ^{-1}}$。 Bert-whitening使用的是PCA白化操作，其实可以进一步再承上$U$，完成ZCA白化操作，转换至原始特征空间。 Bert-whitening即不需叠加训练的方式，也达到了和flow相同的水准，还降低了维度，可谓一举三得。 带参Bert-whitening 上文提到白化操作最终推导公式为： x~i=(xi−μ)UΛ−1\r \\tilde{x}_{i}=(x_{i}- \\mu)U \\sqrt{\\Lambda ^{-1}}\r ​x​~​​​i​​=(x​i​​−μ)U√​Λ​−1​​​​​ 从评测中可知有些数据集评测指标相比first-last-avg是下降的，还不如不标准正交。解决之道：增加两个超参数，控制转换力度。 x~i=(xi−βμ)UΛ−γ/2\r \\tilde{x}_{i}=(x_{i}- \\beta \\mu)U \\Lambda ^{- \\gamma /2}\r ​x​~​​​i​​=(x​i​​−βμ)UΛ​−γ/2​​ 这样总有一种力度能达到最佳。 CoSent Sbert存在的问题是训练和预测的不一致，而如果直接优化预测目标cos，效果往往很差。解决之道：利用排序学习的方式，使得正样本对的距离都小于负样本对的距离。 log(1+∑sin(i,j)>sin(k,l)eλ(cos(uk,ul)−cos(ui,uj)))\r \\log(1+ \\sum _{\\sin(i,j)> \\sin(k,l)}e^{\\lambda(\\cos(u_{k},u_l)- \\cos(u_{i},u_{j}))})\r log(1+∑​sin(i,j)>sin(k,l)​​e​λ(cos(u​k​​,u​l​​)−cos(u​i​​,u​j​​))​​) def cosent_loss(y_true, y_pred): \"\"\"排序交叉熵 y_true：标签/打分，y_pred：句向量 \"\"\" y_true = y_true[::2, 0] y_true = K.cast(y_true[:, None] sin(k,l) y_pred = K.concatenate([[0], y_pred], axis=0) # e^0=1 return K.logsumexp(y_pred) 引用 （2019）BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding （2019）Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks （2019）Improving Neural Language Generation With Spectrum Control （2019）Representation Degeneration Problem In Training Natural Language Generation Models （2020）On the Sentence Embeddings from Pre-trained Language Models Flow-based Generative Model （2021）Whitening Sentence Representations for Better Semantics and Faster Retrieval （2022.5）带参bert-whitening：https://spaces.ac.cn/archives/9079 （2022.1）CoSENT：https://kexue.fm/archives/8847 关注本公众号，下期更精彩 By 杜永安，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2022-11-19 11:46:25 "},"相似度计算/对比学习生成句子向量的最新进展.html":{"url":"相似度计算/对比学习生成句子向量的最新进展.html","title":"对比学习生成句子向量的最新进展","keywords":"","body":"对比学习生成句子向量的最新进展 目录 [TOC] SimBert simbert以UniLM为模型结构，UniLM融合了NLU和NLG，NLU部分使用双向transformer，NLG部分采用单向transformer。如下图： simbert同时训练相似句的生成和匹配，匹配部分直接使用一个batch内其他样本作为负样本，整个batch的CLS组成的矩阵norm之后取内积可得到（batch_size，batch_size）的矩阵，利用softmax进行分类（注意对角线掩码）。训练的数据采集自百度知道，利用简单的方式构建相似问，形成有监督训练的样本。 def compute_loss_of_similarity(self, inputs, mask=None): _, _, y_pred, _ = inputs y_true = self.get_labels_of_similarity(y_pred) # 构建标签 y_pred = K.l2_normalize(y_pred, axis=1) # 句向量归一化 similarities = K.dot(y_pred, K.transpose(y_pred)) # 相似度矩阵 similarities = similarities - K.eye(K.shape(y_pred)[0]) * 1e12 # 排除对角线 similarities = similarities * 30 # scale loss = K.categorical_crossentropy( y_true, similarities, from_logits=True ) return loss SimBert v2 SimBERT=BERT+UniLM+对比学习 RoFormer-Sim=RoFormer+UniLM+对比学习+BART+蒸馏 模型结构换成了RoFormer 训练数据同样来源百度知道 同一个问题的答案是相似的（强） 同一篇章的句子是相似的（弱） 470万一般句子，3000万问句 随机token替换[MASK]，BART是“输入带噪声的句子，输出原句子”，SimBertv2是“输入带噪声的句子，输出原句子的一个相似句” 以上方法增强了生成能力，却降低了匹配能力，也许是因为引入噪声语义不准确或与预测阶段不同，解决之道：RoFormer-Sim训练完后，通过蒸馏把SimBERT的检索效果转移给RoFormer-Sim。对于同一批句子，SimBERT句向量$u1,u2,⋯,un$，RoFormer-Sim为$v1,v2,⋯,vn$，则： Lsim=λn2∑i=1n∑j=1n(cos(ui,uj)−cos(vi,vj))2\r \\mathcal L _ { s i m } = \\frac { \\lambda } { n ^ { 2 } } \\sum _ { i = 1 } ^ { n } \\sum _ { j = 1 } ^ { n } ( \\cos ( u _ { i } , u _ { j } ) - \\cos ( v _ { i } , v _ { j } ) ) ^ { 2 }\r L​sim​​=​n​2​​​​λ​​∑​i=1​n​​∑​j=1​n​​(cos(u​i​​,u​j​​)−cos(v​i​​,v​j​​))​2​​ ​ 为了防止生成能力遗忘，损失函数为L=Lsin+Lgen\\mathcal L=\\mathcal L_{\\sin}+\\mathcal L_{gen}L=L​sin​​+L​gen​​ ，作者蒸馏训练了5000 step RoFormer主要的idea 提出了一种旋转式位置编码RoPE 更好的处理长文本和线性Attention Bart在引入噪声方面的贡献： 随机将token替换成[MASK]（原bert） 随机删去token 随机将一段连续的token（称作span）替换成一个[MASK]，span的长度服从 λ=3 的泊松分布。span长度为0等价于插入一个[MASK]。 将一个document的句子打乱 从document序列中随机选择一个token作为document的开头 ConSERT 数据增强方式 打乱词序（Token Shuffling）：将Position Ids进行Shuffle 裁剪（Cutoff） Token Cutoff：随机将Token Embedding整行置零。 Feature Cutoff：随机将Embedding的Feature维度整列置为零。 Dropout：Embedding层中每个元素以一定概率置为零。 以下数据增强方式由于不一定保持语义一致且开销大，未使用： 回译：利用机器翻译模型，将文本翻译到另一个语言，再翻译回来。 CBERT：将部分词替换成[MASK]，利用BERT去恢复，生成增强句子。 意译：利用相似句生成模型生成同义句。 数据增强消融实验结果： 组合：Token Shuffle和Feature Cutoff（72.74）。 单个：Token Shuffle > Token Cutoff >> Feature Cutoff ≈ Dropout >> None 训练目标 一个batch内由数据增强的句子作为正例，其他作为负例 Li,j=−logexp(cos(ri,rj)/τ)∑k=12N1[k≠i]∣exp(cos(ri,rk)/τ)\r \\mathcal L_{i,j}=- \\log \\frac{exp(\\cos(r_{i},r_{j})/ \\tau)}{\\sum _{k=1}^{2N}1_{[k\\neq i]}|exp(\\cos(r_{i},r_{k})/ \\tau)}\r L​i,j​​=−log​∑​k=1​2N​​1​[k≠i]​​∣exp(cos(r​i​​,r​k​​)/τ)​​exp(cos(r​i​​,r​j​​)/τ)​​ τ\\tauτ是一个超参数，ConSERT取0.1，k≠ik\\neq ik≠i排除对角线。 SimCSE ConSERT只对Embedding层进行干扰，SimCSE对每一层都进行干扰，但SimCSE干扰方式只有Dropout一种。 无监督学习和ConSERT无差，不再赘述。 有监督采用度量学习正负例组成三元组(xi,xi+,xi−)(x_{i},x_{i}^{+},x_{i}^{-})(x​i​​,x​i​+​​,x​i​−​​)，损失函数为： −logesin(hi,hi+)/τ∑j=1N(esin(hi,hj+)/τ+esin(hi,hj−)/τ)\r - \\log \\frac{e^{\\sin(h_{i},h_{i}^{+})/ \\tau}}{\\sum _{j=1}^{N}(e^{\\sin(h_{i},h_{j}^{+})/ \\tau}+e^{\\sin(h_{i},h_{j}^{-})/ \\tau})}\r −log​∑​j=1​N​​(e​sin(h​i​​,h​j​+​​)/τ​​+e​sin(h​i​​,h​j​−​​)/τ​​)​​e​sin(h​i​​,h​i​+​​)/τ​​​​ ESimCSE SimCSE 用Dropout干扰无法影响句子长度，使得模型认为句子长度相仿的语义更相似，ESimCSE使用重复单词的方法改变句子长度。 另外单独用一个encoder作为动量encoder，此encoder参数的更新方式为（论文中$\\lambda 取 0.995 $）： θm=λθm+(1−λ)θe\r \\theta _{m}= \\lambda \\theta _{m}+(1- \\lambda)\\theta _{e}\r θ​m​​=λθ​m​​+(1−λ)θ​e​​ 使用一个定长队列存储历史负例，不断更新这个队列，增加模型可见的负例。 PromptBert 主要思想：通过不同模板产生的语义向量构造正样本，同一批次中的其他样本作为负样本。 损失函数，为了消除Prompt模板影响，会减去模板语义向量，$h{i}-\\hat{h}{i}$和$hi^{\\prime}-\\hat{h}{i}^{\\prime}$分别是两种模板生成的句子 Li=−logecos(hi−h^i,hi′−h^i′)/τ∑j=1Necos(hi−h^i,hj′−h^j′)/τ\r \\mathcal L_{i}=-\\log\\frac{e^{\\cos(h_{i}-\\hat{h}_{i} ,h_i^{\\prime}-\\hat{h}_{i}^{\\prime})/\\tau}}{ \\sum_{j=1}^{N}e^{\\cos(h_{i}-\\hat{h}_{i}, h_{j}^{\\prime}-\\hat{h}_{j}^{\\prime})/\\tau}}\r L​i​​=−log​∑​j=1​N​​e​cos(h​i​​−​h​^​​​i​​,h​j​′​​−​h​^​​​j​′​​)/τ​​​​e​cos(h​i​​−​h​^​​​i​​,h​i​′​​−​h​^​​​i​′​​)/τ​​​​ 例如【吃饭了的意思是 [mask] 】 [mask]位置的向量减去 ​ 【[x] [x] [x] 的意思是 [mask]】 [mask]位置的向量 注意利用[x] 补位 引用 （2020.5）simbert：https://kexue.fm/archives/7427 （2021.3）ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer （2021.6）RoFormer-Sim（simbert v2）：https://spaces.ac.cn/archives/8454 （2022.3）SimCSE: Simple Contrastive Learning of Sentence Embeddings （2022.8）PromptBERT: Improving BERT Sentence Embeddings with Prompts （2022.9）ESimCSE: Enhanced Sample Building Method for Contrastive Learning of Unsupervised Sentence Embedding 关注本公众号，下期更精彩 By 杜永安，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2022-11-19 11:21:57 "},"相似度计算/由词向量word2vec、fastext如何表征句子向量.html":{"url":"相似度计算/由词向量word2vec、fastext如何表征句子向量.html","title":"由词向量Word2vec、Fastext如何表征句子向量","keywords":"","body":"由词向量word2vec、fastext如何表征句子向量 [TOC] word2vec 两种训练方式 训练的目标是单词预测，但输出单词向量的是隐藏层 CBOW：根据上下文预测当前值 Skip-gram：根据当前词预测上下文 注意：skip-gram的训练过程不是一次性用中心词预测四个词，而是中心词和一个周围词组成一个训练样本 加快训练速度的两种方法 采样率 P(wi)=(z(wi)0.001+1)⋅0.001z(wi)\r P(w_{i})=(\\sqrt{\\frac{z(w_{i})}{0.001}}+1)\\cdot \\frac{0.001}{z(w_{i})}\r P(w​i​​)=(√​​0.001​​z(w​i​​)​​​​​+1)⋅​z(w​i​​)​​0.001​​ ​ $z(w_i)$：单词$w_i$出现的次数与总单词个数的比值 ​ $P(w_i)$：是保留该单词的概率 ​ ​ 由图可知单词$w_i$出现的次数越多，保留概率越低，避免重复无用的训练 负采样率 P(wi)=f(wi)3/4∑j=0n(f(wj)3/4)\r P(w_{i})= \\frac{f(w_{i})^{3/4}}{\\sum _{j=0}^{n}(f(w_{j})^{3/4})}\r P(w​i​​)=​∑​j=0​n​​(f(w​j​​)​3/4​​)​​f(w​i​​)​3/4​​​​ ​ $f(w_i)$：词频 ​ 词频越高被负采样概率越高。 层次softmax ​ Huffman Tree （最优二叉树：最重要的放在最前面） ​ 图b为最优二叉树，带权路径长度计算： ​ 图a：$WPL = 5 2 + 7 2 + 2 2 +13 2 = 54$ ​ 图b：$WPL = 5 3 + 2 3 + 7 2 + 13 1 = 48$ ​ 最优二叉树的构造过程如下图： ​ 编码方法：左子树为0，右子树为1 ​ 所以D编码为0，B编码为10，C编码为110，A编码为111 ​ 那么输出层的训练目标可以从词典数量缩减到最优二叉树的深度了，用$sigmoid$二分类预测每一位编码。 优点：使用向量时直接KV取值，速度快 缺点： ​ 1. 无法解决一词多义问题 ​ 2. OOV：超出词典无法表征，没有词序====>解决之道【FastText】 ​ 3. 句子内n-grams，天生具有词序信息 “我 爱 她”如果加入 2-Ngram，加入特征 “我-爱” 和 “爱-她”，“我 爱 她” 和 “她 爱 我” 就能区别开了。 Hash解决n-gram膨胀问题，如n-gram数为10240，设置n-gram最大词典数为1024，取余再转向量 Hash冲突问题实际对效果影响不大 ​ 4. 词内n-grams，即subword，解决未登录词的问题。 1. 平均 句子$S$中所有词向量加起来求平均 V=∑wi∈SvwiN\r V=\\frac{\\sum_{w_i \\in S} v_{w_i}}{N}\r V=​N​​∑​w​i​​∈S​​v​w​i​​​​​​ 2. 加权平均 TFIDF作为权重，对词向量加权求平均 V=∑wi∈S(TF⋅IDF)vwiN\r V=\\frac{\\sum_{w_i \\in S} (TF \\cdot IDF) v_{w_i}}{N}\r V=​N​​∑​w​i​​∈S​​(TF⋅IDF)v​w​i​​​​​​ TF⋅IDF=count(w)∣Di∣⋅logN1+∑i=1NI(w,Di)\r TF \\cdot IDF = \\frac{count(w)}{|D_{i}|} \\cdot \\log \\frac{N}{1+ \\sum _{i=1}^{N}I(w,D_{i})}\r TF⋅IDF=​∣D​i​​∣​​count(w)​​⋅log​1+∑​i=1​N​​I(w,D​i​​)​​N​​ $count(w)$：文档$D_i$中词$w$的数量。 $|D_i|$：文档$D_i$中所有词的数量。 $N$：文档总数。 $I(w,Di)$：文档Di是否包含关键词，若包含则为1，若不包含则为0。 3. SIF 加权平均换为平滑逆词频，a为调节参数 w=aa+TF\r w=\\frac{a}{a+TF}\r w=​a+TF​​a​​ 加权平均之后再减去句子矩阵经过SVD的主成分 SVD图示 代码理解 from sklearn.decomposition import TruncatedSVD X=np.random.random((20,128)) svd = TruncatedSVD(n_components=1, n_iter=7, random_state=0) svd.fit(X) pc = svd.components_ X = X - X.dot(pc.transpose()) * pc pc.transpose().shape #(128, 1) X.dot(pc.transpose()).shape #(20, 1) pc.shape #(1, 128) 4. doc2vec doc2vec与word2vec一样也有两种训练方式CBOW和Skip-gram。Doc2vec与word2vec的不同在于，在输入层增加了一个句子向量Paragraph vector，在同一个句子的若干次训练中是共享的，可以看作句子的主旨。 引用 Efficient estimation of word representations in vector space 统计自然语言处理 关注本公众号，下期更精彩 By 杜永安，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2022-11-19 11:14:28 "},"知识图谱/KBQA新方法.html":{"url":"知识图谱/KBQA新方法.html","title":"KBQA新方法","keywords":"","body":"KBQA新方法 以往的KBQA步骤范式为： 实体识别 实体链接 意图识别（or 关系or 属性识别） 查询数据库 而本文中介绍的来自美团1的方案是： 实体识别 实体链接 查询数据库（控制步数） 关系学习模型打分 引子 接下来将详细介绍论文中的方法，但在介绍之前，先看看2021年3月份一篇关于知识补全的文章BERTRL2： 与以往将知识图谱用Trans系列或图神经网络建模的方式不同，文章中将知识图谱线性化输入到Bert中。文章中提及了两种线性化图谱的方式： 将所有涉及待预测量两个实体之间的路径同时输入模型 分别将每种路径单独输入模型，之后分数聚合（文中的聚合方式为取分数最大值） score(h,r,t)=maxP(y=1∣h,r,t,hpt,t)\r { s c o r e } ( h , r , t ) = { max } P ( y = 1 | h , r , t , h \\frac { p } { t } , t )\r score(h,r,t)=maxP(y=1∣h,r,t,h​t​​p​​,t) 这里举一个输入的例子：[CLS]姚明的妻子是谁？是叶莉吗？[SEP]姚明的女儿是姚沁蕾；姚沁蕾的妈妈是叶莉 如此一来，就融合了知识图谱的知识以及语义知识了。提到这，是不是想起被Jena知识推理工具支配的恐惧，Jena的知识推理是基于经验主义的规则系统，召回率极低，但准确率高，代码举例： [rule: (?a :女儿 ?b)(?b :妈妈 ?c) -> (?a :夫妻 ?c)] 有关jena的工程化实践，有时间会细说。 主文 前文说过有两种线性化图谱的方式，BERTRL用了单独输入各个路径的方式，而美团这篇用了另一种，平铺一次性输入，举例： [CLS]姚明的妻子是谁？是叶莉吗？[SEP] 姚明的女儿是姚沁蕾；姚沁蕾的妈妈是叶莉 [SEP] 姚明的母亲是方凤娣；方凤娣的儿媳是叶莉 微调方法 将全部路径一股脑输入模型，预测档期那候选实体是否为答案，之后依次将所有候选实体算出分数，即达到了目的。 预训练方法 除了直接使用Bert模型进行进行微调之外，作者还提供了一种预训练的方法，预训练之后再进行微调可有效提高KBQA效果。作者提出了三种任务： 关系抽取 [CLS]句子[SEP]头实体h, 关系r, 尾实体t[SEP] 根据句子预测头实体h和尾实体t是否具有关系r 关系匹配 [CLS]句子1 [SEP]句子2 [SEP] 预测句子1和句子2是否具有相同的关系 关系推理即BERTRL 引用 Large-Scale Relation Learning for Question Answering over Knowledge Bases with Pre-trained Language Models Inductive Relation Prediction by BERT 关注本公众号，下期更精彩 By 杜永安，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2022-11-19 11:14:55 "},"自然语言处理/你还在用TextRank or TFIDF 抽取关键词吗？.html":{"url":"自然语言处理/你还在用TextRank or TFIDF 抽取关键词吗？.html","title":"你还在用TextRank Or TFIDF 抽取关键词吗？","keywords":"","body":"什么！！！你还在用TextRank or TFIDF 抽取关键词吗？ 本文着眼于简略地介绍关键词提取技术的前世今生 回顾历史 无监督 统计模型 FirstPhrases TfIdf KPMiner (El-Beltagy and Rafea, 2010) YAKE (Campos et al., 2020) 图模型 TextRank (Mihalcea and Tarau, 2004) SingleRank (Wan and Xiao, 2008) TopicRank (Bougouin et al., 2013) TopicalPageRank (Sterckx et al., 2015) PositionRank (Florescu and Caragea, 2017) MultipartiteRank (Boudin, 2018) 有监督 Kea (Witten et al., 2005) TFIDF '' count(w)∣Di∣⋅logN1+∑i=1NI(w,Di)\r \\frac { { count } ( w ) } { | D _ { i } | } \\cdot \\log \\frac { N } { 1 + \\sum _ { i = 1 } ^ { N } I ( w , D _ { i } ) }\r ​∣D​i​​∣​​count(w)​​⋅log​1+∑​i=1​N​​I(w,D​i​​)​​N​​ 名字以及包含含义即即 【词频】 乘以 【文档频率的倒数】（取对数） 词频等于 【该词出现次数】 除以 【本篇文章词总数】 文档频率 等于 【该词出现在多少文章中】 除以 【文章总数】 (1为了防止分母为0) TextRank S(Vi)=1−d+d⋅∑j∈In(vi)1∣out(vj)∣S(Vj)\r S(V_{i})=1-d+d \\cdot \\sum _{j \\in In(v_{i})}\\frac{1}{|out(v_{j})|}S(V_{j})\r S(V​i​​)=1−d+d⋅∑​j∈In(v​i​​)​​​∣out(v​j​​)∣​​1​​S(V​j​​) 在TextRank提取关键词算法中，限定窗口大小，构建词语共现网络，此时可构建无权无向图，也可根据出现次序构建无权有向图，根据PageRank算法迭代算出权重。实验证明无权无向图效果更好。（d是阻尼因子，防止外链为0的点出现Dead Ends问题） WS(Vi)=(1−d)+d∗∑j=1n(Vi)∑j=NkWiWjikWS(Vj)\r W S ( V _ { i } ) = ( 1 - d ) + d * \\sum _ { j = 1 n ( V _ { i } ) } \\sum _ { j = N _ { k } } ^ { W _ { i } } W _ { j i k } W S ( V _ { j } )\r WS(V​i​​)=(1−d)+d∗∑​j=1n(V​i​​)​​∑​j=N​k​​​W​i​​​​W​jik​​WS(V​j​​) 而TextRank提取摘要算法中，构建的是有权无向图，节点是句子，权重是相似度，相似度的计算如下： similarity(SiSj)=∣{Wk}∣Wk∈Si8Wk∈Sj∣log(∣Si∣)+log(∣Si∣)\r { s i m i l a r i t y } ( S _ { i } S _ { j } ) = \\frac { | \\left\\{W_{k}\\right\\} |W_{k}\\in S_{i}8W_{k}\\in S_{j}| } { \\log ( | S _ { i } | ) + \\log ( | S _ { i } | ) }\r similarity(S​i​​S​j​​)=​log(∣S​i​​∣)+log(∣S​i​​∣)​​∣{W​k​​}∣W​k​​∈S​i​​8W​k​​∈S​j​​∣​​ 分子是两句子共同的词数量，分母是两个句子词数对数求和。 走进新时代 bert也可以用来抽取关键词 如图所示，将句子输入BERT模型，得到句子向量再与各个词的向量做余弦距离，得出关键词。 sini=cos(wi,W)\r \\sin_{i}= \\cos(w_{i},W)\r sin​i​​=cos(w​i​​,W) 使用起来也非常简单： ! pip install keybert from keybert import KeyBERT import jieba_fast from tkitKeyBertBackend.TransformersBackend import TransformersBackend from transformers import BertTokenizer, BertModel doc = \"\" seg_list = jieba_fast.cut(doc, cut_all=True) doc = \" \".join(seg_list) tokenizer = BertTokenizer.from_pretrained('uer/chinese_roberta_L-2_H-128') model = BertModel.from_pretrained(\"uer/chinese_roberta_L-2_H-128\") custom_embedder = TransformersBackend(embedding_model=model,tokenizer=tokenizer) kw_model = KeyBERT(model=custom_embedder) kw_model.extract_keywords(doc, keyphrase_ngram_range=(1, 1), stop_words=None) .png) 抽取关键词还可以预训练 本篇论文最大的贡献在于提出了一种关键词生成的预训练手段，并且证明了此预训练方法对于其他下游任务如NER, QA, RE，summarization也有提升效果。 具体的，如上图所示， LKBIR(θ)=αLMLM(θ)+γLInfil(θ)+σLLP(θ)+δLKRC(θ)\r \\mathcal{L}_{KBIR}(\\theta)= \\alpha \\mathcal{L}_{MLM}(\\theta)+ \\gamma \\mathcal{L}_{Infil}(\\theta)+\\sigma \\mathcal{L}_{LP}(\\theta)+ \\delta \\mathcal{L}_{KRC}(\\theta)\r L​KBIR​​(θ)=αL​MLM​​(θ)+γL​Infil​​(θ)+σL​LP​​(θ)+δL​KRC​​(θ) MLM：masked language model（masked Token Prediction） 单字符掩码任务 Infil：Keyphrase infilling 关键词填充任务，类似于 SpanBERT，掩码于整个关键词 LP：Length Prediction，关键词长度预测，将长度预测作为分类任务 KRC： Keyphrase Replacement Classification 随机替换同类关键词，二分类是否被替换了 使用方法 from transformers import ( TokenClassificationPipeline, AutoModelForTokenClassification, AutoTokenizer, ) from transformers.pipelines import AggregationStrategy import numpy as np class KeyphraseExtractionPipeline(TokenClassificationPipeline): def __init__(self, model, *args, **kwargs): super().__init__( model=AutoModelForTokenClassification.from_pretrained(model), tokenizer=AutoTokenizer.from_pretrained(model), *args, **kwargs ) def postprocess(self, model_outputs): results = super().postprocess( model_outputs=model_outputs, aggregation_strategy=AggregationStrategy.SIMPLE, ) return np.unique([result.get(\"word\").strip() for result in results]) model_name = \"ml6team/keyphrase-extraction-kbir-inspec\" extractor = KeyphraseExtractionPipeline(model=model_name) 引用 统计学习方法第2版 Self-supervised Contextual Keyword and Keyphrase Retrieval with Self-Labelling Learning Rich Representation of Keyphrases from Text PKE: an open source python-based keyphrase extraction toolkit. Capturing Global Informativeness in Open Domain Keyphrase Extraction 关注本公众号，下期更精彩 By 杜永安，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2022-11-19 11:24:03 "},"语言模型/anttention为什么除以根号dk.html":{"url":"语言模型/anttention为什么除以根号dk.html","title":"Anttention为什么除以根号Dk","keywords":"","body":"没有比这更详细的推导 attention为什么除以根号dk——深入理解Bert系列文章 目录 [TOC] 维持均值为0，方差为1的分布 基础知识 随机变量 对随机事物的量化。例如硬币正反面是随机的，用0和1表示正反面，就成了随机数。 期望 E(aX)=aE(X)E(aX)=aE(X)E(aX)=aE(X) 连续型期望 E[X]=∫−∞∞xf(x)dxE [ X ] = \\int _ { - \\infty } ^ { \\infty } x f ( x ) d xE[X]=∫​−∞​∞​​xf(x)dx 离散型期望 E(X)=∑i=1∞xipxiE ( X ) = \\sum _ { i = 1 } ^ { \\infty } x _ { i } p _ { x_i }E(X)=∑​i=1​∞​​x​i​​p​x​i​​​​ 条件期望 随机变量X的条件期望$E(X|Y=y$) 依赖于Y的值y，即 $E(X|Y=y)$是y的函数 ，则$E(X|Y)$是Y的函数 。 举例：投硬币，正面朝上的概率为Y，投掷次数为n，正面朝上次数为X，则 $E(X|Y=y)=ny$，得出 $E(X|Y)=nY$ ，因此$E(X|Y)$也是随机变量。 条件期望 离散型 E(X∣Y=y)=∑x∈XxP(X=x∣Y=y)=∑x∈XxP(X=x,Y=y)P(Y=y)E ( X | Y = y ) = \\sum _ { x \\in \\mathcal X } x P ( X = x | Y = y ) = \\sum _ {x \\in \\mathcal X} x \\frac { P ( X = x , Y = y ) } { P ( Y = y ) }E(X∣Y=y)=∑​x∈X​​xP(X=x∣Y=y)=∑​x∈X​​x​P(Y=y)​​P(X=x,Y=y)​​ 连续型 E(X∣Y=y)=∫XxfX(X∣Y=y)dxE ( X | Y = y ) = \\int _ { \\mathcal X} x f_{\\mathcal X} ( X | Y = y ) d xE(X∣Y=y)=∫​X​​xf​X​​(X∣Y=y)dx 条件期望的期望 离散型 E[E[X∣Y]]E [ E [ X | Y ] ]E[E[X∣Y]]=∑yPY(y)E[X[Y=y]=\\sum _ { y } P_Y ( y )E [ X [ Y = y ] =∑​y​​P​Y​​(y)E[X[Y=y] 连续型 E[E[X∣Y]]E [ E [ X | Y ] ]E[E[X∣Y]]=∫−∞∞E[X∣Y=y]fY(y)dy=\\int _ { - \\infty } ^ { \\infty } E [ X | Y = y ] f _ { Y } ( y ) d y=∫​−∞​∞​​E[X∣Y=y]f​Y​​(y)dy 迭代期望法则 又名重期望法则：E[E[X∣Y]]=E[X]E [ E [ X | Y ] ] = E [ X ]E[E[X∣Y]]=E[X]，即条件期望的期望等于无条件期望。（根据全期望定理可得出） 方差 $Var(aX)=a^{2}Var(X)$ var[X]=E[(X−E(X))2] { v a r } [ X ] = E [ ( X - E ( X ) ) ^ { 2 } ]var[X]=E[(X−E(X))​2​​] =E[X2+E[X]2−2XE[X]]\\qquad \\quad = E [ X ^ { 2 } + E [ X ] ^ { 2 } - 2 X E [ X ] ]=E[X​2​​+E[X]​2​​−2XE[X]] =E[X2]+E[X]2−2E[X]2\\qquad \\quad= E [ X ^ { 2 } ] + E [ X ] ^ { 2 } - 2 E [ X ] ^ { 2 }=E[X​2​​]+E[X]​2​​−2E[X]​2​​ =E[X2]−E[X]2\\qquad \\quad= E [ X ^ { 2 } ] - E [ X ] ^ { 2 }=E[X​2​​]−E[X]​2​​ 【公式一】 协方差 cov[X,Y]=E[(X−E(X))(Y−E(Y))] { c o v } [ X , Y ] = E [ ( X - E ( X ) ) ( Y - E ( Y ) ) ]cov[X,Y]=E[(X−E(X))(Y−E(Y))] =E[XY−XE[Y]−E[X]Y+E[X]E[Y]]\\qquad \\qquad = E [ X Y - X E [ Y ] - E [ X ] Y + E [ X ] E [ Y ] ]=E[XY−XE[Y]−E[X]Y+E[X]E[Y]] =E[XY]−E[X]E[Y]−E[X]E[Y]+E[X]E[Y]E[Y]\\qquad \\qquad = E [ X Y ] - E [ X ] E [ Y ] - E [ X ] E [ Y ] + E [ X ] E [ Y ] E [ Y ]=E[XY]−E[X]E[Y]−E[X]E[Y]+E[X]E[Y]E[Y] =E[XY]−E[X]E[Y]\\qquad \\qquad = E [ X Y ] - E [ X ] E [ Y ]=E[XY]−E[X]E[Y] 【公式二】 同变量的协方差等于方差：cov[X,X]=var[X]cov [ X , X ] = { v a r } [ X ]cov[X,X]=var[X] 两个分布独立的变量协方差为0。 随机变量点积 期望 E[XY]=E[E[XY∣Y]]E [ X Y ] =E [ E [ X Y | Y ] ]E[XY]=E[E[XY∣Y]]=E[YE[X∣Y]]= E [ Y E [ X | Y ] ]=E[YE[X∣Y]] 迭代期望法则 E[E[XY∣Y]]E [ E [ X Y| Y ] ]E[E[XY∣Y]]=∑yPY(y)E[Xy[Y=y]=\\sum _ { y } P_Y ( y )E [ Xy [ Y = y ] =∑​y​​P​Y​​(y)E[Xy[Y=y] =∑yPY(y)yE[X[Y=y]\\qquad \\qquad \\quad =\\sum _ { y } P_Y ( y )yE [ X [ Y = y ] =∑​y​​P​Y​​(y)yE[X[Y=y] 常数提前 =E[YE[X∣Y]]\\qquad \\qquad \\quad = E [ Y E [ X | Y ] ]=E[YE[X∣Y]] 当X和Y分布独立E[X∣Y]=E[X]E [ X | Y ] = E [ X ]E[X∣Y]=E[X]，则：E[XY]=E[Y⋅E[X]]E [ X Y ] = E [ Y \\cdot E [ X ] ]E[XY]=E[Y⋅E[X]]=E[X]⋅E[Y]= E [ X ] \\cdot E [ Y ]=E[X]⋅E[Y] 方差 var[XY]=E[X2Y2]−E[XY]2 { v a r } [ X Y ] = E [ X ^ { 2 } Y ^ { 2 } ] - E [ X Y ] ^ { 2 }var[XY]=E[X​2​​Y​2​​]−E[XY]​2​​ 根据公式一 和公式二 ： E[X2Y2]=cov[X2,Y2]+E[X2]E[Y2]E [ X ^ { 2 } Y ^ { 2 } ] = cov [ X ^ { 2 } , Y ^ { 2 } ] + E [ X ^ { 2 } ] E [ Y ^ { 2 } ]E[X​2​​Y​2​​]=cov[X​2​​,Y​2​​]+E[X​2​​]E[Y​2​​] =cos[X2,Y2]+(E[X]2+vax[X])⋅(E[Y]2+var[Y])\\qquad \\qquad = \\cos [ X ^ { 2 } , Y ^ { 2 } ] + ( E [ X ] ^ { 2 } + v a x [ X ] ) \\cdot ( E [ Y ] ^ { 2 } + v a r [ Y ] )=cos[X​2​​,Y​2​​]+(E[X]​2​​+vax[X])⋅(E[Y]​2​​+var[Y]) E[XY]2=(cotX,Y]+E[X]E[Y])2E [ X Y ] ^ { 2 } = ( \\cot X , Y ] + E [ X ] E [ Y ] ) ^ { 2 }E[XY]​2​​=(cotX,Y]+E[X]E[Y])​2​​ 则var[XY] { v a r } [ X Y ]var[XY] 等于 var[XY]=cot[X2,Y2]+(E[X]2+var[X])⋅(E[Y]2+var[Y])−(cov[X,Y]+E[X]E[Y])2 { v a r } [ X Y ] = \\cot [ X ^ { 2 } , Y ^ { 2 } ] + ( E [ X ] ^ { 2 } + { v a r } [ X ] ) \\cdot ( E [ Y ] ^ { 2 } + { v a r } [ Y ] ) - ( c o v [ X , Y ] + E [ X ]E [ Y ] ) ^ { 2 }var[XY]=cot[X​2​​,Y​2​​]+(E[X]​2​​+var[X])⋅(E[Y]​2​​+var[Y])−(cov[X,Y]+E[X]E[Y])​2​​ 当X和Y分布独立cos[X2,Y2]=cos[X,Y]=0\\cos [ X ^ { 2 } , Y ^ { 2 } ] = \\cos [ X , Y ] = 0cos[X​2​​,Y​2​​]=cos[X,Y]=0，则： var[XY]=(E[X]2+var[X])⋅(E[Y]2+var[Y])−(E[X]E[Y])2 { v a r } [ X Y ] = ( E [ X ] ^ { 2 } + { v a r } [ X ] ) \\cdot ( E [ Y ] ^ { 2 } + { v a r } [ Y ] ) - ( E [ X ] E [ Y ] ) ^ { 2 }var[XY]=(E[X]​2​​+var[X])⋅(E[Y]​2​​+var[Y])−(E[X]E[Y])​2​​ =E[X]2var[Y]+E[Y]2[X]+var[X][Y]\\qquad \\qquad = E [ X ] ^ { 2 } { v a r } [ Y ] + E [ Y ] ^ { 2 } [ X ] + { v a r } [ X ] [ Y ]=E[X]​2​​var[Y]+E[Y]​2​​[X]+var[X][Y] 因为在这里X和Y是以0为均值的，所以var[XY]=var[X]var[Y] { v a r } [ X Y ] = { v a r } [ X ] { v a r } [ Y ]var[XY]=var[X]var[Y] 随机变量的和 设Z是n个随机变量的和，Z=∑i=1nXiZ = \\sum _ { i = 1 } ^ { n } X _ { i }Z=∑​i=1​n​​X​i​​ 期望 E[Z]=∑i=1nE[Xi]E [ Z ] = \\sum _ { i = 1 } ^ { n } E [ X _ { i } ]E[Z]=∑​i=1​n​​E[X​i​​] 方差 var(Z)=cov[∑i=1nXi , ∑j=1nXj] { v a r } ( Z ) = { c o v } [ \\sum _ { i = 1 } ^ { n } X _ { i } \\space ,\\space \\sum _ { j = 1 } ^ { n } X _ { j }]var(Z)=cov[∑​i=1​n​​X​i​​ , ∑​j=1​n​​X​j​​] =∑i=1n∑j=1ncos[Xi,Xj]\\qquad \\quad = \\sum _ { i = 1 } ^ { n } \\sum _ { j = 1 } ^ { n } \\cos [ X _ { i } , X _ { j } ]=∑​i=1​n​​∑​j=1​n​​cos[X​i​​,X​j​​] 当$X_i$互相独立： var(Z)=∑i=1ncov[Xi,Xi] { v a r } ( Z ) = \\sum _ { i = 1 } ^ { n } cov [ X _ { i } , X _ { i } ]var(Z)=∑​i=1​n​​cov[X​i​​,X​i​​] =∑i=1nvar[Xi]\\qquad \\quad = \\sum _ { i = 1 } ^ { n } { v a r } [ X _ { i } ]=∑​i=1​n​​var[X​i​​] 随机向量点击 设q和k是两个$d_k$维的向量，并且每一维是独立的，且 E[qi]=E[ki]=0E [ q _ { i } ] = E [ k _ { i } ] = 0E[q​i​​]=E[k​i​​]=0 var[qi]=var[ki]=1 { v a r } [ q _ { i } ] = { v a r } [ k _ { i } ] = 1var[q​i​​]=var[k​i​​]=1 i∈[0,dk]i \\in [ 0 , d _ { k } ]i∈[0,d​k​​] 那么： E[q⋅k]=E[∑i=1dkqiki]E [ q \\cdot k ] = E [ \\sum _ { i = 1 } ^ { d _ { k } } q _ { i } k _ { i } ]E[q⋅k]=E[∑​i=1​d​k​​​​q​i​​k​i​​] =∑i=1dkE[qiki)\\qquad \\quad = \\sum _ { i = 1 } ^ { d _ { k } } E [ q _ { i } k _ { i } )=∑​i=1​d​k​​​​E[q​i​​k​i​​) =∑i=1dkE[qi]E[ki]\\qquad \\quad= \\sum _ { i = 1 } ^ { d _ { k } } E [ q _ { i } ] E [ k _ { i } ]=∑​i=1​d​k​​​​E[q​i​​]E[k​i​​] =0\\qquad \\quad=0=0 var[q⋅k]=var[∑i=1dkqiki]{ v a r } [ q \\cdot k ] = { v a r } [ \\sum _ { i = 1 } ^ { d _ { k } }q_ik_i ]var[q⋅k]=var[∑​i=1​d​k​​​​q​i​​k​i​​] =∑i=1dkvar[qiki]\\qquad \\quad= \\sum _ { i = 1 } ^ { d _ { k } } { v a r } [ q _ { i } k _ { i } ]=∑​i=1​d​k​​​​var[q​i​​k​i​​] =∑i=1dkvar[qi][ki]\\qquad \\quad= \\sum _ { i = 1 } ^ { d _ { k } } { v a r } [ q _ { i } ] [ k _ { i } ]=∑​i=1​d​k​​​​var[q​i​​][k​i​​] =∑i=1dkvar[qi][ki]\\qquad \\quad= \\sum _ { i = 1 } ^ { d _ { k } } { v a r } [ q _ { i } ] [ k _ { i } ]=∑​i=1​d​k​​​​var[q​i​​][k​i​​] =∑i=1dk1\\qquad \\quad= \\sum _{i=1}^{d_{k}}1=∑​i=1​d​k​​​​1 =dk\\qquad \\quad= d _ { k }=d​k​​ 则： var[q⋅kdk]=1dkvar[∑i=1dkqiki] { v a r } [\\frac{q \\cdot k }{\\sqrt d_k} ] ={\\frac{1 }{ d_k}} { v a r } [ \\sum _ { i = 1 } ^ { d _ { k } }q_ik_i ]var[​√​d​​​​k​​​​q⋅k​​]=​d​k​​​​1​​var[∑​i=1​d​k​​​​q​i​​k​i​​] $=1$ 引用 Statistical-Properties-of-Dot-Product/proof.pdf at master · BAI-Yeqi/Statistical-Properties-of-Dot-Product (github.com) 关注本公众号，下期更精彩 By 杜永安，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2022-11-19 11:15:11 "}}