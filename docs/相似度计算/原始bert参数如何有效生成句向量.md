# åŸå§‹bertå‚æ•°å¦‚ä½•æœ‰æ•ˆç”Ÿæˆå¥å‘é‡



**ç›®å½•**

[TOC]



## BERT

![image-20221010141651121](https://notebook-media.oss-cn-beijing.aliyuncs.com/img/image-20221010141651121.png)

### CLS

ç›´æ¥ä½¿ç”¨[CLS]ä½ç½®å‘é‡ä½œä¸ºå¥å‘é‡                                                   



### last average

æœ€åä¸€å±‚è¯å‘é‡ï¼ˆå³é™¤[CLS]ä¹‹å¤–ï¼‰æ±‚å¹³å‡



### first last average

ç¬¬ä¸€å±‚å’Œæœ€åä¸€å±‚è¯å‘é‡å¹³å‡



### SBERT

![image-20221012110535823](https://notebook-media.oss-cn-beijing.aliyuncs.com/img/image-20221012110535823.png)

sberté‡‡ç”¨ç»å…¸çš„åŒå¡”æ–¹å¼ï¼Œæå‡ºäº†ä¸‰ç§ç›®æ ‡å‡½æ•°ï¼š

1. æ‹¼æ¥ä¸¤ä¸ªå¥å­å‘é‡ï¼Œç›´æ¥äºŒåˆ†ç±»ï¼ˆClassification Objective Functionï¼‰
   $$
   softmax(W_{t}(u,v,|u-v|))
   $$
   é€‰æ‹©$(u,v,|u-v|)$æ‹¼æ¥ï¼Œmean poolingæ–¹å¼æ•ˆæœæœ€å¥½

   ![image-20221012155027719](https://notebook-media.oss-cn-beijing.aliyuncs.com/img/image-20221012155027719.png)

2. åˆ©ç”¨ä¸¤ä¸ªå¥å­çš„ä½™å¼¦è·ç¦»çš„mean-squared-errorä½œä¸ºæŸå¤±å‡½æ•°ï¼ˆRegression Objective Functionï¼‰

   ä»¥ä¸‹ä¸ºè®ºæ–‡ä¸­çš„å®ç°

   

   ```python
   class CosineSimilarityLoss(nn.Module):
       def __init__(self, model: SentenceTransformer, loss_fct = nn.MSELoss(), cos_score_transformation=nn.Identity()):
           super(CosineSimilarityLoss, self).__init__()
           self.model = model
           self.loss_fct = loss_fct
           self.cos_score_transformation = cos_score_transformation
   
   
       def forward(self, sentence_features: Iterable[Dict[str, Tensor]], labels: Tensor):
           embeddings = [self.model(sentence_feature)['sentence_embedding'] for sentence_feature in sentence_features]
           output = self.cos_score_transformation(torch.cosine_similarity(embeddings[0], embeddings[1]))
           return self.loss_fct(output, labels.view(-1))
   ```

   

3. æ¯æ¬¡ç»™ä¸€ä¸ªæ­£ä¾‹ï¼Œä¸€ä¸ªè´Ÿä¾‹ï¼Œå¢å¤§è´Ÿä¾‹è·ç¦»ï¼Œå‡å°æ­£ä¾‹è·ç¦»ï¼ˆTriplet Objective Functionï¼‰ã€‚$s_x$ä¸ºå¥å­å‘é‡$||\cdot||$æ˜¯ä¸€ç§è·ç¦»è¡¡é‡ï¼Œé—´éš” $\epsilon$ ä¿è¯æ­£ä¾‹$s_p$(positive) æ¯”è´Ÿä¾‹ $s_n$(negative) è‡³å°‘è¦è¿‘$\epsilon$ã€‚

$$
\max(||s_{a}-s_{p}||-||s_{a}-s_{n}||+ \epsilon ,0)
$$



#### åœ¨STSæ•°æ®çš„æµ‹è¯„

##### æ— ç›‘ç£

åœ¨æ— ç›‘ç£é¢†åŸŸå®éªŒè¡¨æ˜æ— è®ºæ˜¯bert-clsè¿˜æ˜¯bert-avgæ•ˆæœéƒ½è¿˜ä¸å¦‚ç›´æ¥Glove

> GloVeä¸word2vecçš„åŒºåˆ«
>
> word2vecï¼šæ ¹æ®ä¸Šä¸‹æ–‡å‘¢é¢„æµ‹ä¸­é—´çš„è¯æ±‡ï¼Œæˆ–è€…æ ¹æ®ä¸­é—´çš„è¯æ±‡é¢„æµ‹ä¸Šä¸‹æ–‡ã€‚
>
> GloVeï¼šæ„å»ºè¯æ±‡çš„å…±ç°çŸ©é˜µï¼Œå†è¿›è¡Œç±»ä¼¼PCAçš„æ“ä½œ

![image-20221012162242596](https://notebook-media.oss-cn-beijing.aliyuncs.com/img/image-20221012162242596.png)

##### æœ‰ç›‘ç£

ä½œè€…ä½¿ç”¨Regression Objective FunctionæŸå¤±å‡½æ•°çš„æ–¹å¼è¿›è¡Œè®­ç»ƒå¾—å‡ºä»¥ä¸‹æµ‹è¯„ï¼š

![image-20221012164309419](https://notebook-media.oss-cn-beijing.aliyuncs.com/img/image-20221012164309419.png)



### Bert-flow

ä»ä¸Šæ–‡ä¸­æˆ‘ä»¬çŸ¥é“ï¼Œbertåœ¨æ²¡æœ‰fine-turnçš„æƒ…å†µä¸‹ï¼Œæ— è®ºä½¿ç”¨CLSè¿˜æ˜¯avgæ•ˆæœéƒ½å¾ˆå·®ã€‚ä¸ºå•¥å‘¢ï¼Ÿ

1. CLSé¢„è®­ç»ƒçš„ç›®æ ‡æ˜¯åˆ¤æ–­ä¸¤ä¸ªå¥å­æ˜¯å¦å­˜åœ¨å»¶ç»­æ€§ï¼Œã€è¿ç»­æ€§å’Œç›¸ä¼¼å¤§ç›¸å¾„åº­ã€‘ã€‚
2. å„å‘å¼‚æ€§ï¼šå„ä¸ªç»´åº¦è¡¡é‡æ ‡å‡†ä¸ä¸€æ ·(å·¦å›¾ä¸ºå„å‘å¼‚æ€§ï¼Œå³å›¾ä¸ºå„å‘åŒæ€§)

<img src="https://notebook-media.oss-cn-beijing.aliyuncs.com/img/v2-287a4e8478b38ac7dd401c503ddb97c0_720w.png" style="zoom: 67%;" />

â€‹		ç”±ä¸‹å›¾å¯çŸ¥word2vecå„å‘åˆ†å¸ƒè¾ƒä¸ºä¸€è‡´ï¼Œè€Œtransformerå·®å¼‚æ€§å¾ˆå¤§ã€‚

![image-20221012173359353](https://notebook-media.oss-cn-beijing.aliyuncs.com/img/image-20221012173359353.png)

â€‹		å„å‘å¼‚æ€§ä¼šå¯¼è‡´å‘é‡å †ç§¯åœ¨ä¸€èµ·ï¼Œä½™å¼¦ç›¸ä¼¼åº¦éƒ½å¾ˆé«˜

![](https://notebook-media.oss-cn-beijing.aliyuncs.com/img/v2-16afeb2bcb31948dded07183b983b609_720w.jpg)

3. bert ç©ºé—´å‘é‡æ‰€å±çš„åæ ‡ç³»å¹¶éæ ‡å‡†æ­£äº¤åŸºã€‚
   $$
   \cos(x,y)= \frac{\sum _{i=1}^{d}x_{i}y_{i}}{\sqrt{\sum _{i=1}^{d}x_{i}}\sqrt{\sum _{i=1}^{d}y_{i}}}
   $$
   ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—æ–¹å¼æ˜¯åœ¨æ ‡å‡†æ­£äº¤åŸºä¸‹æˆç«‹çš„ï¼Œå¦‚æœå¦‚æœç¬¦åˆæ ‡å‡†æ­£äº¤åŸºï¼Œè¯­ä¹‰å­¦ä¹ çš„å¥½ï¼Œæ•ˆæœåº”è¯¥æ˜¯å¥½çš„ã€‚

   > æ ‡å‡†æ­£äº¤åŸºï¼šä¸€ä¸ªå†…ç§¯ç©ºé—´çš„æ­£äº¤åŸºï¼ˆorthogonal basisï¼‰æ˜¯å…ƒç´ ä¸¤ä¸¤æ­£äº¤çš„åŸºã€‚ç§°åŸºä¸­çš„å…ƒç´ ä¸ºåŸºå‘é‡ã€‚å‡è‹¥ï¼Œä¸€ä¸ªæ­£äº¤åŸºçš„åŸºå‘é‡çš„æ¨¡é•¿éƒ½æ˜¯å•ä½é•¿åº¦1ï¼Œåˆ™ç§°è¿™æ­£äº¤åŸºä¸ºæ ‡å‡†æ­£äº¤åŸº



å¦‚ä½•è§£å†³å‘¢ï¼Ÿ

bert-flowæå‡ºçš„è§£å†³æ–¹æ¡ˆæ˜¯ä»å„å‘å¼‚æ€§å…¥æ‰‹çš„ï¼šå°†åŸå§‹çš„bertå¥å­å‘é‡ç©ºé—´è½¬æ¢åˆ°æ ‡å‡†é«˜æ–¯åˆ†å¸ƒç©ºé—´ã€‚

![image-20221012174752657](https://notebook-media.oss-cn-beijing.aliyuncs.com/img/image-20221012174752657.png)

$u$æ˜¯bertç©ºé—´å‘é‡ï¼ˆobserve spaceï¼‰ï¼Œ$z$æ˜¯é«˜æ–¯ç©ºé—´å‘é‡ï¼ˆlatent spaceï¼‰ï¼Œæœä»$p_z(z)$çš„å…ˆéªŒåˆ†å¸ƒã€‚$f$æ˜¯é€šè¿‡ã€**è®­ç»ƒ**ã€‘ï¼ˆå›ºå®šbertå‚æ•°ï¼Œåªè¿­ä»£é¢„è®­ç»ƒè½¬åŒ–å‡½æ•°å‚æ•°ï¼‰å¾—åˆ°çš„ä»é«˜æ–¯ç©ºé—´è½¬æ¢åˆ°bertç©ºé—´çš„å¯é€†å‡½æ•°ã€‚
$$
z \sim p_z(z),u=f_{\phi}(z)
$$

æ ¹æ®æµæ¨¡å‹ï¼ˆflow modelï¼‰åŸç†ï¼Œbertç©ºé—´è½¬æ¢ä¸ºé«˜æ–¯ç©ºé—´å¦‚ä¸‹ï¼ˆ$det$ï¼šè¡Œåˆ—å¼ï¼›$f^{-1}$ï¼š$f$ çš„é€†å‡½æ•°ï¼‰ï¼š
$$
p_u(u)=p_z(f_{\phi}^{-1}(u))|det \frac{\partial f_{\phi}^{-1}(u)}{\partial u}|
$$
æœ€å¤§åŒ–å¯¹æ•°ä¼¼ç„¶å‡½æ•°ï¼ˆlog likelihoodï¼‰
$$
\max_{\phi} \mathbb E_u=BERT(sentence),sentence \sim \mathcal D
$$

$$
\log p_z(f_{\phi}^{-1}(u))+ \log |det \frac{\partial f_{\phi}^{-1}(u)}{\partial u}|
$$

#### æµæ¨¡å‹ï¼ˆflow modelï¼‰

![Snipaste_2022-10-13_11-53-22](https://notebook-media.oss-cn-beijing.aliyuncs.com/img/Snipaste_2022-10-13_11-53-22.png)

å®šä¹‰åŸå§‹æ•°æ®åˆ†å¸ƒä¸ºç®€å•çš„é«˜æ–¯åˆ†å¸ƒ$\pi(z)$ï¼Œä¸”$ğ‘¥=G(z)$ ã€‚è½¬æ¢ä¹‹åçš„xæœä»$p_G(x)$åˆ†å¸ƒã€‚æ­¤æ—¶çš„ç›®æ ‡å‡½æ•°æœ€å¤§åŒ–å¯¹æ•°ä¼¼ç„¶ç­‰äºæœ€å°åŒ–KLæ•£åº¦ã€‚

 **ä¸€äº›åŸºç¡€çŸ¥è¯†**

 1. é›…å¯æ¯”çŸ©é˜µï¼ˆJacobianï¼‰

    å®šä¹‰$ğ‘¥=f(z)$ ,$f$ä¸ºå¯é€†å‡½æ•°å³$z=f^{-1}(x)$ã€‚å¦‚æœè®¾xå’Œzéƒ½æ˜¯äºŒç»´çš„ï¼Œé‚£ä¹ˆæ­¤æ—¶Jacobian matrixï¼ˆä»¥ä¸‹ç®€ç§°$J_f$ï¼‰å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚

    æ‰€ä»¥å¦‚æœ$z=[z_1,z_2],x=[z_1+z_2,2z_2]$ï¼Œæ ¹æ®$J_{f}$çš„æ±‚åå¯¼çš„è§„åˆ™å¯æ±‚å¾—$J_{f}$ ï¼ŒåŒç†å¯å¾—$J_{f^{-1}}$ï¼ŒåŒæ—¶å¯çŸ¥$J_{f}J_{f^{-1}}=I$ã€‚

![image-20221013151032918](https://notebook-media.oss-cn-beijing.aliyuncs.com/img/image-20221013151032918.png)

â€‹    

 2. è¡Œåˆ—å¼ï¼ˆDeterminantï¼‰

    è®¡ç®—æ–¹å¼ï¼š$$D= \sum(-1)^{k}a_{1k_{1}}a_{2k_{2}}\cdots a_{nk}$$

    

    

![image-20221013151354679](https://notebook-media.oss-cn-beijing.aliyuncs.com/img/image-20221013151354679.png)

â€‹    

â€‹    

è¡Œåˆ—å¼çš„å‡ ä½•æ„ä¹‰å…¶å®æ˜¯é«˜ç»´ç©ºé—´çš„â€œä½“ç§¯â€ã€‚


â€‹    

â€‹    

![image-20221013151312730](https://notebook-media.oss-cn-beijing.aliyuncs.com/img/image-20221013151312730.png)

â€‹    

 3. éšæœºå˜é‡çš„å˜é‡æ›¿æ¢å®šç†(Change of Variable Theorem)

    å®šä¹‰zæœä»$\pi(z)$ï¼Œxæœä»$p(x)$åˆ†å¸ƒï¼Œ$ğ‘¥=f(z)$ ã€‚xå¯ä»¥zé€šè¿‡få¾—åˆ°ï¼Œé‚£ä¹ˆ$\pi(z)$å’Œ$p(x)$æœ‰ä»€ä¹ˆå…³ç³»å‘¢ï¼Ÿ

    

![image-20221013151446650](https://notebook-media.oss-cn-beijing.aliyuncs.com/img/image-20221013151446650.png)

â€‹    

å‡è®¾$\pi(z)$å’Œ$p(x)$éƒ½æœä»æœ€ç®€å•çš„å‡åŒ€åˆ†å¸ƒï¼ˆUniform Distributionï¼‰ï¼Œ$\pi(z)\sim U(0,1) \ ,\  p(x)\sim U(1,3)$ã€‚ä¸ºäº†ä¿è¯ç§¯åˆ†é¢ç§¯éƒ½ä¸º1ï¼Œåˆ™$\pi(z)=1\ ,\ p(x)=0.5$ï¼Œå³$p(x^{\prime})= \frac{1}{2}\pi(z^{\prime})$ã€‚


â€‹    

![image-20221013151609476](https://notebook-media.oss-cn-beijing.aliyuncs.com/img/image-20221013151609476.png)

â€‹    

æ›´ä¸€èˆ¬çš„æƒ…å†µï¼Œå¤æ‚çš„åˆ†å¸ƒå¯ä»¥å–å…¶ä¸­ä¸€å°æ®µè§†ä¸ºå‡åŒ€åˆ†å¸ƒï¼Œç”±æ­¤å¯ä»¥å¾—åˆ°$p(x^{\prime})= \pi(z^{\prime})| \frac{dz}{dx}|$ã€‚ï¼ˆæœ‰å¯èƒ½åå‘å¯¹åº”ï¼Œè¦åŠ ç»å¯¹å€¼ï¼‰


â€‹    

![image-20221013151641573](https://notebook-media.oss-cn-beijing.aliyuncs.com/img/image-20221013151641573.png)

â€‹    

â€‹    

æ¨å¹¿åˆ°äºŒç»´çš„æƒ…å†µï¼ˆåˆ«å¿˜äº†ï¼Œdetçš„å«ä¹‰æ˜¯é¢ç§¯ï¼‰ï¼š



![image-20221013151844217](https://notebook-media.oss-cn-beijing.aliyuncs.com/img/image-20221013151844217.png)

â€‹    

ç»è¿‡ä»¥ä¸‹çš„æ¨åˆ°å¯ä»¥å¾—åˆ°ä¸¤ä¸ªåˆ†å¸ƒä¹‹é—´çš„æ¨å¯¼å…¬å¼ï¼š


â€‹    

![image-20221013151937288](https://notebook-media.oss-cn-beijing.aliyuncs.com/img/image-20221013151937288.png)

â€‹    

æ‰€ä»¥å°†ä¼¼ç„¶å‡½æ•°ä¸­$P_G$æ›¿æ¢æˆåˆ†å¸ƒä¹‹é—´çš„æ¨å¯¼å…¬å¼ï¼Œå°†$z^i$æ›¿æ¢ä¸ºå‘é‡ä¹‹é—´çš„æ¨å¯¼å…¬å¼ï¼Œå¯å¾—åˆ°æœ€ç»ˆçš„ç›®æ ‡å‡½æ•°ã€‚


â€‹    

![image-20221013152047399](https://notebook-media.oss-cn-beijing.aliyuncs.com/img/image-20221013152047399.png)







### Bert-whitening

bert-flowä¸ºäº†èƒ½å¤Ÿè½¬åŒ–åˆ†å¸ƒï¼Œä»ç„¶éœ€è¦è®­ç»ƒä¸€ä¸ªè½¬æ¢å‚æ•°ã€‚Bert-whiteningé‡‡ç”¨äº†æ›´åŠ ç›´æ¥çš„åŠæ³•ï¼Œä¸éœ€è¦å†å»è¿­ä»£é¢„è®­ç»ƒï¼Œä»æ ‡å‡†æ­£äº¤åŸºå…¥æ‰‹ï¼Œåˆ©ç”¨ç™½åŒ–çš„æ‰‹æ®µå°†bertå‘é‡è½¬æ¢åˆ°ä»¥æ ‡å‡†æ­£äº¤åŸºä¸ºåŸºåº•çš„å‘é‡ã€‚

![image-20221014010817361](https://notebook-media.oss-cn-beijing.aliyuncs.com/img/image-20221014010817361.png)

ç»è¿‡ç™½åŒ–æ“ä½œçš„æ•°æ®å…·æœ‰ä»¥ä¸‹ç‰¹å¾ï¼š

- æ¶ˆé™¤äº†ç‰¹å¾ä¹‹é—´çš„ç›¸å…³æ€§ï¼ˆåæ–¹å·®çŸ©é˜µé™¤å¯¹è§’çº¿éƒ½ä¸º0ï¼‰
- æ‰€æœ‰ç‰¹å¾çš„æ–¹å·®éƒ½ä¸º 1ï¼ˆåæ–¹å·®çŸ©é˜µå¯¹è§’çº¿éƒ½ä¸º1ï¼‰

> PCAç™½åŒ–ï¼š$W= \Lambda ^{-1/2}U^{T}$
>
> ZCAç™½åŒ–ï¼š$W=U \Lambda ^{-1/2}U^{T}$
>
> åŒï¼šéƒ½æ¶ˆé™¤äº†ç‰¹å¾ä¹‹é—´ç›¸å…³æ€§ï¼ŒåŒæ—¶ä½¿å¾—æ‰€æœ‰ç‰¹å¾çš„æ–¹å·®éƒ½ä¸º 1ã€‚
>
> å¼‚ï¼šç›¸æ¯”äºPCAç™½åŒ–ï¼ŒZCAç™½åŒ–æŠŠæ•°æ®æ—‹è½¬å›äº†åŸæ¥çš„ç‰¹å¾ç©ºé—´ï¼Œå¤„ç†åçš„æ•°æ®æ›´æ¥è¿‘åŸå§‹æ•°æ®ã€‚

è®¾å‘é‡é›†åˆä¸º$\left\{x_{i}\right\} _{i=1}^{N}$ï¼Œæ ‡å‡†çš„é«˜æ–¯åˆ†å¸ƒå‡å€¼ä¸º0ï¼Œåæ–¹å·®ä¸ºå•ä½é˜µè¿™ä¸ç™½åŒ–æ“ä½œä¸è°‹è€Œåˆã€‚æƒ³è¦å‡å€¼ä¸º0ç›´æ¥$x_i-\mu $ï¼Œå…¶ä¸­$\mu= \frac{1}{N}\sum _{i=1}^{N}x_{i}$ ã€‚æ¥ä¸‹æ¥å°†$x_i-\mu $ æ‰§è¡Œçº¿æ€§å˜åŒ–$(x_{i}- \mu)W$ï¼Œä½¿å¾—è½¬æ¢åçš„æ•°æ®åæ–¹å·®çŸ©é˜µä¸ºå•ä½é˜µã€‚
$$
\tilde{x}_{i}=(x_{i}- \mu)W
$$

åœ¨ç™½åŒ–æ“ä½œä¸­ï¼Œ$W$ç»™å‡ºäº†è§£æ³•ï¼š

çŸ©é˜µ$X$ç™½åŒ–è‡³çŸ©é˜µ$Y$ï¼Œæ ¹æ®åæ–¹å·®æ±‚è§£å…¬å¼ï¼Œ$X$åæ–¹å·®$D_X=\frac{1}{m}XX^{T}$ï¼Œæ‰€ä»¥ï¼š
$$
D_Y= \frac{1}{m}YY^{T}= \frac{1}{m}(WX)(WX)^{T}
$$

$$
= \frac{1}{m}WXX^{T}W^{T}=W(\frac{1}{m}XX^{T})W^{T}=WD_XW^{T}
$$

ç™½åŒ–çš„ç›®æ ‡æ˜¯ä½¿å¾—$\tilde{\Sigma}=W^{T}\Sigma W=I$ï¼Œæ‰€ä»¥å¯ä»¥æ¨å¯¼å‡ºï¼š
$$
 \Rightarrow \Sigma =(W^{T})^{-1}W^{-1}=(W^{-1})^{T}W^{-1}
$$
$\Sigma$æ˜¯ä¸€ä¸ªåŠæ­£å®šå¯¹ç§°çŸ©é˜µï¼Œæ•°æ®å¤Ÿå¤šæ—¶é€šå¸¸æ˜¯æ­£å®šçš„ï¼Œä»è€Œ$\Sigma=U \Lambda U^{T}$ï¼Œ$U$æ˜¯æ­£äº¤çŸ©é˜µï¼Œè€Œ$\Lambda$æ˜¯å¯¹è§’é˜µï¼Œå¹¶ä¸”å¯¹è§’çº¿å…ƒç´ éƒ½æ˜¯æ­£çš„ï¼Œé‚£ä¹ˆå¯ä»¥ä»¤$W^{-1}= \sqrt{\Lambda}U^{T}$,å³$W=U \sqrt{\Lambda ^{-1}}$ã€‚

Bert-whiteningä½¿ç”¨çš„æ˜¯PCAç™½åŒ–æ“ä½œï¼Œå…¶å®å¯ä»¥è¿›ä¸€æ­¥å†æ‰¿ä¸Š$U$ï¼Œå®ŒæˆZCAç™½åŒ–æ“ä½œï¼Œè½¬æ¢è‡³åŸå§‹ç‰¹å¾ç©ºé—´ã€‚

Bert-whiteningå³ä¸éœ€å åŠ è®­ç»ƒçš„æ–¹å¼ï¼Œä¹Ÿè¾¾åˆ°äº†å’Œflowç›¸åŒçš„æ°´å‡†ï¼Œè¿˜é™ä½äº†ç»´åº¦ï¼Œå¯è°“ä¸€ä¸¾ä¸‰å¾—ã€‚


![image-20221013175410120](https://notebook-media.oss-cn-beijing.aliyuncs.com/img/image-20221013175410120.png)





## å¸¦å‚Bert-whitening

ä¸Šæ–‡æåˆ°ç™½åŒ–æ“ä½œæœ€ç»ˆæ¨å¯¼å…¬å¼ä¸ºï¼š
$$
\tilde{x}_{i}=(x_{i}- \mu)U \sqrt{\Lambda ^{-1}}
$$
ä»è¯„æµ‹ä¸­å¯çŸ¥æœ‰äº›æ•°æ®é›†è¯„æµ‹æŒ‡æ ‡ç›¸æ¯”first-last-avgæ˜¯ä¸‹é™çš„ï¼Œè¿˜ä¸å¦‚ä¸æ ‡å‡†æ­£äº¤ã€‚è§£å†³ä¹‹é“ï¼šå¢åŠ ä¸¤ä¸ªè¶…å‚æ•°ï¼Œæ§åˆ¶è½¬æ¢åŠ›åº¦ã€‚
$$
\tilde{x}_{i}=(x_{i}- \beta \mu)U \Lambda ^{- \gamma /2}
$$
è¿™æ ·æ€»æœ‰ä¸€ç§åŠ›åº¦èƒ½è¾¾åˆ°æœ€ä½³ã€‚



## CoSent

Sbertå­˜åœ¨çš„é—®é¢˜æ˜¯è®­ç»ƒå’Œé¢„æµ‹çš„ä¸ä¸€è‡´ï¼Œè€Œå¦‚æœç›´æ¥ä¼˜åŒ–é¢„æµ‹ç›®æ ‡cosï¼Œæ•ˆæœå¾€å¾€å¾ˆå·®ã€‚è§£å†³ä¹‹é“ï¼šåˆ©ç”¨æ’åºå­¦ä¹ çš„æ–¹å¼ï¼Œä½¿å¾—æ­£æ ·æœ¬å¯¹çš„è·ç¦»éƒ½å°äºè´Ÿæ ·æœ¬å¯¹çš„è·ç¦»ã€‚
$$
\log(1+ \sum _{\sin(i,j)> \sin(k,l)}e^{\lambda(\cos(u_{k},u_l)- \cos(u_{i},u_{j}))})
$$


<img src="https://notebook-media.oss-cn-beijing.aliyuncs.com/img/image-20221021163231894.png" alt="image-20221021163231894" style="zoom: 33%;" />



```python
def cosent_loss(y_true, y_pred):
    """æ’åºäº¤å‰ç†µ
    y_trueï¼šæ ‡ç­¾/æ‰“åˆ†ï¼Œy_predï¼šå¥å‘é‡
    """
    y_true = y_true[::2, 0]
    y_true = K.cast(y_true[:, None] < y_true[None, :], K.floatx())
    y_pred = K.l2_normalize(y_pred, axis=1)
    y_pred = K.sum(y_pred[::2] * y_pred[1::2], axis=1) * 20
    y_pred = y_pred[:, None] - y_pred[None, :]  #cos(u_{k},u_l)- cos(u_{i},u_{j})
    y_pred = K.reshape(y_pred - (1 - y_true) * 1e12, [-1])  #ç­›é€‰sin(i,j)> sin(k,l)
    y_pred = K.concatenate([[0], y_pred], axis=0)  # e^0=1
    return K.logsumexp(y_pred)
```







## å¼•ç”¨

1. ï¼ˆ2019ï¼‰BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
1. ï¼ˆ2019ï¼‰Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks
1. ï¼ˆ2019ï¼‰Improving Neural Language Generation With Spectrum Control
1. ï¼ˆ2019ï¼‰Representation Degeneration Problem In Training Natural Language Generation Models
1. ï¼ˆ2020ï¼‰On the Sentence Embeddings from Pre-trained Language Models
1. Flow-based  Generative Model 
1. ï¼ˆ2021ï¼‰Whitening Sentence Representations for Better Semantics and Faster Retrieval
1. ï¼ˆ2022.5ï¼‰å¸¦å‚bert-whiteningï¼šhttps://spaces.ac.cn/archives/9079
1. ï¼ˆ2022.1ï¼‰CoSENTï¼šhttps://kexue.fm/archives/8847





<font size=24>å…³æ³¨æœ¬å…¬ä¼—å·ï¼Œä¸‹æœŸæ›´ç²¾å½©</font>

<img src="https://notebook-media.oss-cn-beijing.aliyuncs.com/img/image-20220930221129484.png" alt="image-20220930221129484" style="zoom: 80%;" />





